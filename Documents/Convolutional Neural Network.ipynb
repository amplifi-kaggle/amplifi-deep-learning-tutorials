{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(10000, 784)\n",
      "(10000, 28, 28, 1)\n",
      "step: 0 | accuracy: 0.2867 | learning rate : 0.0100\n",
      "label results:  [7 6 8 6 8 8 8 6 6 7 0 6 8 8 8 6 8 7 6 8]\n",
      "step: 1 | accuracy: 0.2554 | learning rate : 0.0100\n",
      "step: 2 | accuracy: 0.1032 | learning rate : 0.0100\n",
      "step: 3 | accuracy: 0.1031 | learning rate : 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f8229dd5172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0maccuracy_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step: {:01d} | accuracy: {:.4f} | learning rate : {:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "Nowadays\n",
    "\n",
    "Data augmentatino\n",
    "He initialization with ReLU\n",
    "Fully convolutinal architecture\n",
    "     Avoid fully connected layers\n",
    "Batch norm\n",
    "     No dropout\n",
    "     No weight decay\n",
    "Adam optimizer\n",
    "'''\n",
    "\n",
    "# Xavier Initializer\n",
    "xavier_normal_init = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "\n",
    "# He Initializer (Kaiming He)\n",
    "he_normal_init = tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False)\n",
    "\n",
    "# But random gaussian initialization is not good at deep networks\n",
    "# Layers\n",
    "def Dense(input, weight_shape, name='Dense', W_init=he_normal_init, mean=0.0, stddev=0.01, b_init=0.0):\n",
    "    with tf.variable_scope(name):\n",
    "        if W_init == 'Gaussian':\n",
    "            W = tf.Variable(tf.random_normal(weight_shape, mean=mean, stddev=stddev), name=\"W\")\n",
    "        else:\n",
    "            W = tf.get_variable(\"W\", weight_shape, initializer=he_normal_init)\n",
    "        b = tf.get_variable(\"b\", weight_shape[-1], initializer=tf.constant_initializer(value=b_init))\n",
    "        \n",
    "    return tf.matmul(input, W) + b\n",
    "    \n",
    "# Convolutional layer\n",
    "def Conv2D(input, kernel_shape, strides, padding, name='Conv2d', W_init=he_normal_init, mean=0.0, stddev=0.01, b_init=0.0):\n",
    "    with tf.variable_scope(name):\n",
    "        if W_init == 'Gaussian':\n",
    "            W = tf.Variable(tf.random_normal(kernel_shape, mean=mean, stddev=stddev), name=\"W\")\n",
    "        else:\n",
    "            W = tf.get_variable(\"W\", kernel_shape, initializer=W_init)\n",
    "        b = tf.get_variable(\"b\", kernel_shape[-1], initializer=tf.constant_initializer(value=b_init))\n",
    "    return tf.nn.conv2d(input, W, strides, padding) + b\n",
    "    \n",
    "\n",
    "# Regularizations\n",
    "def BatchNorm(input, is_train, decay=0.999, name='BatchNorm'):\n",
    "    '''\n",
    "    https://github.com/zsdonghao/tensorlayer/blob/master/tensorlayer/layers.py\n",
    "    https://github.com/ry/tensorflow-resnet/blob/master/resnet.py\n",
    "    http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow\n",
    "    '''\n",
    "    from tensorflow.python.training import moving_averages\n",
    "    from tensorflow.python.ops import control_flow_ops\n",
    "    \n",
    "    axis = list(range(len(input.get_shape()) - 1))\n",
    "    fdim = input.get_shape()[-1:]\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        beta = tf.get_variable('beta', fdim, initializer=tf.constant_initializer(value=0.0))\n",
    "        gamma = tf.get_variable('gamma', fdim, initializer=tf.constant_initializer(value=1.0))\n",
    "        moving_mean = tf.get_variable('moving_mean', fdim, initializer=tf.constant_initializer(value=0.0), trainable=False)\n",
    "        moving_variance = tf.get_variable('moving_variance', fdim, initializer=tf.constant_initializer(value=0.0), trainable=False)\n",
    "        \n",
    "        def mean_var_with_update():\n",
    "            batch_mean, batch_variance = tf.nn.moments(input, axis)\n",
    "            update_moving_mean = moving_averages.assign_moving_average(moving_mean, batch_mean, decay, zero_debias=True)\n",
    "            update_moving_variance = moving_averages.assign_moving_average(moving_variance, batch_variance, decay, zero_debias=True)\n",
    "            with tf.control_dependencies([update_moving_mean, update_moving_variance]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_variance)\n",
    "\n",
    "        mean, variance = control_flow_ops.cond(is_train, mean_var_with_update, lambda: (moving_mean, moving_variance))\n",
    "\n",
    "    return tf.nn.batch_normalization(input, mean, variance, beta, gamma, 1e-3)\n",
    "\n",
    "# Activations\n",
    "def LeakyReLU(input, alpha=0.2):\n",
    "    return tf.maximum(input, alpha*input)\n",
    "\n",
    "# Define network\n",
    "\n",
    "def Network(t):\n",
    "    t = Conv2D(t, kernel_shape=[3, 3, 1, 16], strides=[1, 1, 1, 1], padding='SAME', name='Conv1')\n",
    "    t = tf.nn.relu(t)\n",
    "    \n",
    "    for i in range(1):\n",
    "        t = Conv2D(t, kernel_shape=[3, 3, 16, 16], strides=[1, 1, 1, 1], padding='SAME', name='Conv' + str(i+2))\n",
    "        t = tf.nn.relu(t)\n",
    "        \n",
    "    t = Conv2D(t, kernel_shape=[3, 3, 16, 4], strides=[1, 1, 1, 1], padding='SAME', name='Conv12')\n",
    "    t = tf.nn.relu(t)\n",
    "    \n",
    "    t = tf.reshape(t, [-1, 28 * 28 * 4])\n",
    "    t = Dense(t, [28*28*4, 10], name='Dense1')\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# read image\n",
    "def np_from_img(fname):\n",
    "    return np.asarray(Image.open(fname), dtype=np.float32)\n",
    "\n",
    "# write image\n",
    "def save_as_img(ar, fname):\n",
    "    Image.fromarray(ar.round().astype(np.uint8)).save(fname)\n",
    "    \n",
    "# normalization\n",
    "def norm(ar):\n",
    "    return 255*np.absolute(ar)/np.max(ar)\n",
    "\n",
    "\n",
    "\n",
    "# placeholder\n",
    "x  = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "learning_rate = tf.placeholder(\"float\", shape=[])\n",
    "\n",
    "# get result of network\n",
    "y = Network(x)\n",
    "\n",
    "# optimization\n",
    "# softmax + cross_entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# make session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# set init learning rate, batch_size\n",
    "batch_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "# load valid data\n",
    "test_xs, test_ys = mnist.test.images, mnist.test.labels\n",
    "print(test_xs.shape)\n",
    "test_xs = np.reshape(test_xs, [-1, 28, 28, 1]) # Reshape data shape\n",
    "print(test_xs.shape)\n",
    "\n",
    "# save 'img_len' valid images\n",
    "str_idx = 0\n",
    "img_len = 20\n",
    "# get first 20 images from test_xs\n",
    "input_image = test_xs[str_idx: str_idx + img_len, :, :, 0]\n",
    "\n",
    "canvas = np.zeros([28, 28 * img_len])\n",
    "\n",
    "for i in range(img_len):\n",
    "    canvas[:, 28 * i : 28 * (i + 1)] = input_image[i, :, :]\n",
    "\n",
    "save_as_img(norm(canvas), './canvas.png')\n",
    "\n",
    "for step in range(1000):\n",
    "    \n",
    "    # step learning policy\n",
    "    if(step % 100 == 0 and step != 0):\n",
    "        lr = lr / 2\n",
    "        \n",
    "    # load datasets\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "    batch_xs = np.reshape(batch_xs, [-1, 28, 28, 1]) # Reshape data shape\n",
    "    \n",
    "    # run session\n",
    "    sess.run(train_step, feed_dict = {x: batch_xs, y_ : batch_ys, learning_rate: lr})\n",
    "    \n",
    "    # validation\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: test_xs, y_: test_ys})\n",
    "    \n",
    "    print('step: {:01d} | accuracy: {:.4f} | learning rate : {:.4f}'.format(step, float(accuracy_), lr))\n",
    "        \n",
    "    if (step % 10 == 0):\n",
    "        mnist_img = np.reshape(input_image, [-1, 28, 28, 1])\n",
    "        mnist_label = sess.run(y, feed_dict={x: mnist_img})\n",
    "        print(\"label results: \", np.argmax(mnist_label, 1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
