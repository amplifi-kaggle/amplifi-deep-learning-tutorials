{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n",
      "b'Hello, Tensorflow'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, Tensorflow')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(c)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "[[ 0.95473993 -0.916623  ]\n",
      " [ 1.07248819 -0.05501784]\n",
      " [-0.31067339  0.35213915]]\n",
      "[[-0.74425435]\n",
      " [ 2.31388235]]\n",
      "[[ 1.42344165 -0.71449554]\n",
      " [ 9.63124275  0.48513603]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.placeholder: 계산을 실행할 때 입력값을 받는 변수로 사용합니다.\n",
    "# None은 크기가 정해지지 않았음을 의미합니다.\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)\n",
    "\n",
    "x_data = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "# tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화합니다.\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "expr = tf.matmul(X, W) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(x_data)\n",
    "print(sess.run(W))\n",
    "print(sess.run(b))\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "step:0 | accuracy : 0.3665\n",
      "step:1 | accuracy : 0.4098\n",
      "step:2 | accuracy : 0.3831\n",
      "step:3 | accuracy : 0.4102\n",
      "step:4 | accuracy : 0.4647\n",
      "step:5 | accuracy : 0.5186\n",
      "step:6 | accuracy : 0.5745\n",
      "step:7 | accuracy : 0.7156\n",
      "step:8 | accuracy : 0.6764\n",
      "step:9 | accuracy : 0.7625\n",
      "step:10 | accuracy : 0.7512\n",
      "step:11 | accuracy : 0.7581\n",
      "step:12 | accuracy : 0.7069\n",
      "step:13 | accuracy : 0.7576\n",
      "step:14 | accuracy : 0.7260\n",
      "step:15 | accuracy : 0.7993\n",
      "step:16 | accuracy : 0.7875\n",
      "step:17 | accuracy : 0.7940\n",
      "step:18 | accuracy : 0.7742\n",
      "step:19 | accuracy : 0.7563\n",
      "step:20 | accuracy : 0.7699\n",
      "step:21 | accuracy : 0.8277\n",
      "step:22 | accuracy : 0.8574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4651f4ec9c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0maccuracy_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step:{:01d} | accuracy : {:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 run_metadata):\n\u001b[1;32m   1116\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1166\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "# 784 input units, 10 hidden units\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# 10 biases attached to hidden units\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_= tf.placeholder(\"float\", [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for step in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {x:batch_xs, y_:batch_ys}) # feed x and y_ to train_step\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,  1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels})\n",
    "    print('step:{:01d} | accuracy : {:.4f}'.format(step, float(accuracy_)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementing the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFirst, we multiply x by W with the expression tf.matmul(x, W). This is flipped from when\\nwe multiplied them in our equation, where we had Wx, as a small trick to deal with x being\\na 2D tensor with multiple inptu.s We then add b, and finally apply tf.nn.softmax.\\n\\nThat's it. It only took us one line to define our model, after a couple short lines of setup.\\n\\nThat isn't because Tensorflow is designed to make a softmax regression particularly \\neasy: it's just a very flexible way to describe many kinds of numerical computations,\\nfrom machine learning models to physics simulations. And once defined, our model can be run\\non different devices: your computer's CPU, GPUs, and even phones!\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To do efficient numerical computing in Python, we typically use libraries like NumPy that\n",
    "do expensive operations such as matrix multiplication outside Python, using highly \n",
    "efficient code implemented in another language. \n",
    "\n",
    "Unfortunately, there can still be a lot of overhead from switching back to Python every operation.\n",
    "This overhead is especially bad if you want to run computations on GPUs or in a distributed manner,\n",
    "where there can be a high cost to transferring data.\n",
    "\n",
    "Instead of running a single expensive operation independently from Python, Tensorflow \n",
    "lets us describe a graph of interacting operations that run entirely outside Python.\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "# we describe these interacting opearations by manipulating symbolic variables.\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "'''\n",
    "x isn't a specific value. It's a placeholder, a value that we'll input when we ask\n",
    "TensorFlow to run a computation. We want to be able to input any number of MNIST images,\n",
    "each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point\n",
    "numbers, with a shape [None, 784]. (Here None means that a dimension can be of any length)\n",
    "\n",
    "We also need the weights and biases for our model. We could imagine treating these like \n",
    "additional inputs, but Tensorflow has an even better way to handle it: Variable. A Variable\n",
    "is a modifiable tensor that lives in TensorFlow's graph of interacting operations.\n",
    "'''\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "'''\n",
    "We create these Variables by giving tf.Variable the initial value of Variable:\n",
    "in this case, we initialize both W and b as tensors full of zeros. Since we are\n",
    "going to learn W and b, it doesn't matter very much what they initially are.\n",
    "\n",
    "Notice that W has a shape of [784, 10] because we want to multiply the 784-dimensional\n",
    "image vectors by it to produce 10-dimensional vectors of evidence for the difference\n",
    "classes. b has a shape of [10] so we can add it to the output.\n",
    "\n",
    "We can now implement our model. It only takes one line to define it!\n",
    "'''\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "'''\n",
    "First, we multiply x by W with the expression tf.matmul(x, W). This is flipped from when\n",
    "we multiplied them in our equation, where we had Wx, as a small trick to deal with x being\n",
    "a 2D tensor with multiple inptu.s We then add b, and finally apply tf.nn.softmax.\n",
    "\n",
    "That's it. It only took us one line to define our model, after a couple short lines of setup.\n",
    "\n",
    "That isn't because Tensorflow is designed to make a softmax regression particularly \n",
    "easy: it's just a very flexible way to describe many kinds of numerical computations,\n",
    "from machine learning models to physics simulations. And once defined, our model can be run\n",
    "on different devices: your computer's CPU, GPUs, and even phones!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One very common, very nice function to determine the loss of a model is called\n",
    "\"cross-entropy.\" Cross-entropy arises from thinking about information compressing codes\n",
    "in information theory, but it winds up being an important idea in lots of areas, from gambling\n",
    "to machine learning. It's defined as:\n",
    "\n",
    "H(y) = - sigma(y' * log(y))\n",
    "\n",
    "where y is our predicted probability distribution, and y' is the true distribution (the one-hot\n",
    "vector with the digit labels.) In some rough sense, the cross-entropy is measuring how inefficient\n",
    "our predictions are for describing the truth.\n",
    "\n",
    "To implement cross-entropy we need to first add a new placeholder to input the correct answers:\n",
    "'''\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Then we can implement the cross-entropy function\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "'''\n",
    "First, tf.log computes the logarithm of each element of y. Next, we multiply each element of y_\n",
    "with the corresponding element of tf.log(y). Then, tf.reduce_sum adds the elements in the\n",
    "second dimension of y, due to the reduction_indices=[1] parameter. Finally, tf.reduce_mean\n",
    "computes the mean over all the examples in the batch.\n",
    "\n",
    "Note that in the source code, we don't use this formulation, because it is numerically \n",
    "unstable. Instead, we apply tf.nn.softmax_cross_entropy_with_logits on the unnormalized\n",
    "logits (e.g., we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b), because\n",
    "this more numerically stable function internally computes the softmax activation. In your \n",
    "code, consider using tf.nn.sofmax_cross_entropy_with_logits instead.\n",
    "\n",
    "Now that we know what we want our model to do, it's very easy to have TensorFlow train\n",
    "it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically\n",
    "use the backpropagation algorithm to efficiently determine how your variables affect\n",
    "the loss you ask it to minimize. Then it can apply your choice of optimization algorithm\n",
    "to modify the variables and reduce the loss\n",
    "'''\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n",
    "\n",
    "'''\n",
    "In this case, we ask Tensorflow to minimize cross_entropy using the graident descent algorithm\n",
    "with a learning rate of 0.5.\n",
    "'''\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "# we first have to create an operation to initialize tha variables we created\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "step: 0 | accuracy : 0.2808 | learning rate : 0.0100\n",
      "step: 1 | accuracy : 0.5905 | learning rate : 0.0100\n",
      "step: 2 | accuracy : 0.4665 | learning rate : 0.0100\n",
      "step: 3 | accuracy : 0.4844 | learning rate : 0.0100\n",
      "step: 4 | accuracy : 0.3135 | learning rate : 0.0100\n",
      "step: 5 | accuracy : 0.5828 | learning rate : 0.0100\n",
      "step: 6 | accuracy : 0.5399 | learning rate : 0.0100\n",
      "step: 7 | accuracy : 0.6283 | learning rate : 0.0100\n",
      "step: 8 | accuracy : 0.5893 | learning rate : 0.0100\n",
      "step: 9 | accuracy : 0.6929 | learning rate : 0.0100\n",
      "step: 10 | accuracy : 0.7203 | learning rate : 0.0100\n",
      "step: 11 | accuracy : 0.6702 | learning rate : 0.0100\n",
      "step: 12 | accuracy : 0.6963 | learning rate : 0.0100\n",
      "step: 13 | accuracy : 0.7262 | learning rate : 0.0100\n",
      "step: 14 | accuracy : 0.7758 | learning rate : 0.0100\n",
      "step: 15 | accuracy : 0.6679 | learning rate : 0.0100\n",
      "step: 16 | accuracy : 0.7339 | learning rate : 0.0100\n",
      "step: 17 | accuracy : 0.7880 | learning rate : 0.0100\n",
      "step: 18 | accuracy : 0.7958 | learning rate : 0.0100\n",
      "step: 19 | accuracy : 0.7988 | learning rate : 0.0100\n",
      "step: 20 | accuracy : 0.7441 | learning rate : 0.0100\n",
      "step: 21 | accuracy : 0.7691 | learning rate : 0.0100\n",
      "step: 22 | accuracy : 0.7431 | learning rate : 0.0100\n",
      "step: 23 | accuracy : 0.7698 | learning rate : 0.0100\n",
      "step: 24 | accuracy : 0.7784 | learning rate : 0.0100\n",
      "step: 25 | accuracy : 0.7759 | learning rate : 0.0100\n",
      "step: 26 | accuracy : 0.8320 | learning rate : 0.0100\n",
      "step: 27 | accuracy : 0.8612 | learning rate : 0.0100\n",
      "step: 28 | accuracy : 0.8571 | learning rate : 0.0100\n",
      "step: 29 | accuracy : 0.8110 | learning rate : 0.0100\n",
      "step: 30 | accuracy : 0.8593 | learning rate : 0.0100\n",
      "step: 31 | accuracy : 0.8419 | learning rate : 0.0100\n",
      "step: 32 | accuracy : 0.7665 | learning rate : 0.0100\n",
      "step: 33 | accuracy : 0.7446 | learning rate : 0.0100\n",
      "step: 34 | accuracy : 0.8078 | learning rate : 0.0100\n",
      "step: 35 | accuracy : 0.8036 | learning rate : 0.0100\n",
      "step: 36 | accuracy : 0.8626 | learning rate : 0.0100\n",
      "step: 37 | accuracy : 0.8804 | learning rate : 0.0100\n",
      "step: 38 | accuracy : 0.8607 | learning rate : 0.0100\n",
      "step: 39 | accuracy : 0.8837 | learning rate : 0.0100\n",
      "step: 40 | accuracy : 0.8667 | learning rate : 0.0100\n",
      "step: 41 | accuracy : 0.8854 | learning rate : 0.0100\n",
      "step: 42 | accuracy : 0.8845 | learning rate : 0.0100\n",
      "step: 43 | accuracy : 0.8648 | learning rate : 0.0100\n",
      "step: 44 | accuracy : 0.8785 | learning rate : 0.0100\n",
      "step: 45 | accuracy : 0.8885 | learning rate : 0.0100\n",
      "step: 46 | accuracy : 0.8836 | learning rate : 0.0100\n",
      "step: 47 | accuracy : 0.8432 | learning rate : 0.0100\n",
      "step: 48 | accuracy : 0.8592 | learning rate : 0.0100\n",
      "step: 49 | accuracy : 0.8792 | learning rate : 0.0100\n",
      "step: 50 | accuracy : 0.8843 | learning rate : 0.0100\n",
      "step: 51 | accuracy : 0.8831 | learning rate : 0.0100\n",
      "step: 52 | accuracy : 0.8645 | learning rate : 0.0100\n",
      "step: 53 | accuracy : 0.8771 | learning rate : 0.0100\n",
      "step: 54 | accuracy : 0.8726 | learning rate : 0.0100\n",
      "step: 55 | accuracy : 0.8862 | learning rate : 0.0100\n",
      "step: 56 | accuracy : 0.8746 | learning rate : 0.0100\n",
      "step: 57 | accuracy : 0.8024 | learning rate : 0.0100\n",
      "step: 58 | accuracy : 0.8231 | learning rate : 0.0100\n",
      "step: 59 | accuracy : 0.8891 | learning rate : 0.0100\n",
      "step: 60 | accuracy : 0.8745 | learning rate : 0.0100\n",
      "step: 61 | accuracy : 0.8859 | learning rate : 0.0100\n",
      "step: 62 | accuracy : 0.8820 | learning rate : 0.0100\n",
      "step: 63 | accuracy : 0.8959 | learning rate : 0.0100\n",
      "step: 64 | accuracy : 0.8477 | learning rate : 0.0100\n",
      "step: 65 | accuracy : 0.8241 | learning rate : 0.0100\n",
      "step: 66 | accuracy : 0.8274 | learning rate : 0.0100\n",
      "step: 67 | accuracy : 0.8639 | learning rate : 0.0100\n",
      "step: 68 | accuracy : 0.8420 | learning rate : 0.0100\n",
      "step: 69 | accuracy : 0.8782 | learning rate : 0.0100\n",
      "step: 70 | accuracy : 0.8885 | learning rate : 0.0100\n",
      "step: 71 | accuracy : 0.8831 | learning rate : 0.0100\n",
      "step: 72 | accuracy : 0.8776 | learning rate : 0.0100\n",
      "step: 73 | accuracy : 0.8782 | learning rate : 0.0100\n",
      "step: 74 | accuracy : 0.8313 | learning rate : 0.0100\n",
      "step: 75 | accuracy : 0.8018 | learning rate : 0.0100\n",
      "step: 76 | accuracy : 0.8922 | learning rate : 0.0100\n",
      "step: 77 | accuracy : 0.8990 | learning rate : 0.0100\n",
      "step: 78 | accuracy : 0.8858 | learning rate : 0.0100\n",
      "step: 79 | accuracy : 0.8833 | learning rate : 0.0100\n",
      "step: 80 | accuracy : 0.8968 | learning rate : 0.0100\n",
      "step: 81 | accuracy : 0.8941 | learning rate : 0.0100\n",
      "step: 82 | accuracy : 0.8909 | learning rate : 0.0100\n",
      "step: 83 | accuracy : 0.8885 | learning rate : 0.0100\n",
      "step: 84 | accuracy : 0.8832 | learning rate : 0.0100\n",
      "step: 85 | accuracy : 0.8770 | learning rate : 0.0100\n",
      "step: 86 | accuracy : 0.8916 | learning rate : 0.0100\n",
      "step: 87 | accuracy : 0.8980 | learning rate : 0.0100\n",
      "step: 88 | accuracy : 0.8529 | learning rate : 0.0100\n",
      "step: 89 | accuracy : 0.8495 | learning rate : 0.0100\n",
      "step: 90 | accuracy : 0.8855 | learning rate : 0.0100\n",
      "step: 91 | accuracy : 0.8726 | learning rate : 0.0100\n",
      "step: 92 | accuracy : 0.8721 | learning rate : 0.0100\n",
      "step: 93 | accuracy : 0.8796 | learning rate : 0.0100\n",
      "step: 94 | accuracy : 0.8861 | learning rate : 0.0100\n",
      "step: 95 | accuracy : 0.8849 | learning rate : 0.0100\n",
      "step: 96 | accuracy : 0.8906 | learning rate : 0.0100\n",
      "step: 97 | accuracy : 0.8814 | learning rate : 0.0100\n",
      "step: 98 | accuracy : 0.8847 | learning rate : 0.0100\n",
      "step: 99 | accuracy : 0.9039 | learning rate : 0.0100\n",
      "step: 100 | accuracy : 0.9033 | learning rate : 0.0010\n",
      "step: 101 | accuracy : 0.9040 | learning rate : 0.0010\n",
      "step: 102 | accuracy : 0.9046 | learning rate : 0.0010\n",
      "step: 103 | accuracy : 0.9051 | learning rate : 0.0010\n",
      "step: 104 | accuracy : 0.9043 | learning rate : 0.0010\n",
      "step: 105 | accuracy : 0.9035 | learning rate : 0.0010\n",
      "step: 106 | accuracy : 0.9039 | learning rate : 0.0010\n",
      "step: 107 | accuracy : 0.9045 | learning rate : 0.0010\n",
      "step: 108 | accuracy : 0.9037 | learning rate : 0.0010\n",
      "step: 109 | accuracy : 0.9046 | learning rate : 0.0010\n",
      "step: 110 | accuracy : 0.9048 | learning rate : 0.0010\n",
      "step: 111 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 112 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 113 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 114 | accuracy : 0.9059 | learning rate : 0.0010\n",
      "step: 115 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 116 | accuracy : 0.9055 | learning rate : 0.0010\n",
      "step: 117 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 118 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 119 | accuracy : 0.9048 | learning rate : 0.0010\n",
      "step: 120 | accuracy : 0.9053 | learning rate : 0.0010\n",
      "step: 121 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 122 | accuracy : 0.9051 | learning rate : 0.0010\n",
      "step: 123 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 124 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 125 | accuracy : 0.9063 | learning rate : 0.0010\n",
      "step: 126 | accuracy : 0.9050 | learning rate : 0.0010\n",
      "step: 127 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 128 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 129 | accuracy : 0.9063 | learning rate : 0.0010\n",
      "step: 130 | accuracy : 0.9055 | learning rate : 0.0010\n",
      "step: 131 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 132 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 133 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 134 | accuracy : 0.9056 | learning rate : 0.0010\n",
      "step: 135 | accuracy : 0.9064 | learning rate : 0.0010\n",
      "step: 136 | accuracy : 0.9059 | learning rate : 0.0010\n",
      "step: 137 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 138 | accuracy : 0.9068 | learning rate : 0.0010\n",
      "step: 139 | accuracy : 0.9055 | learning rate : 0.0010\n",
      "step: 140 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 141 | accuracy : 0.9051 | learning rate : 0.0010\n",
      "step: 142 | accuracy : 0.9054 | learning rate : 0.0010\n",
      "step: 143 | accuracy : 0.9058 | learning rate : 0.0010\n",
      "step: 144 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 145 | accuracy : 0.9055 | learning rate : 0.0010\n",
      "step: 146 | accuracy : 0.9050 | learning rate : 0.0010\n",
      "step: 147 | accuracy : 0.9056 | learning rate : 0.0010\n",
      "step: 148 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 149 | accuracy : 0.9051 | learning rate : 0.0010\n",
      "step: 150 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 151 | accuracy : 0.9062 | learning rate : 0.0010\n",
      "step: 152 | accuracy : 0.9062 | learning rate : 0.0010\n",
      "step: 153 | accuracy : 0.9060 | learning rate : 0.0010\n",
      "step: 154 | accuracy : 0.9055 | learning rate : 0.0010\n",
      "step: 155 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 156 | accuracy : 0.9063 | learning rate : 0.0010\n",
      "step: 157 | accuracy : 0.9064 | learning rate : 0.0010\n",
      "step: 158 | accuracy : 0.9061 | learning rate : 0.0010\n",
      "step: 159 | accuracy : 0.9069 | learning rate : 0.0010\n",
      "step: 160 | accuracy : 0.9063 | learning rate : 0.0010\n",
      "step: 161 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 162 | accuracy : 0.9072 | learning rate : 0.0010\n",
      "step: 163 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 164 | accuracy : 0.9064 | learning rate : 0.0010\n",
      "step: 165 | accuracy : 0.9083 | learning rate : 0.0010\n",
      "step: 166 | accuracy : 0.9073 | learning rate : 0.0010\n",
      "step: 167 | accuracy : 0.9080 | learning rate : 0.0010\n",
      "step: 168 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 169 | accuracy : 0.9077 | learning rate : 0.0010\n",
      "step: 170 | accuracy : 0.9073 | learning rate : 0.0010\n",
      "step: 171 | accuracy : 0.9080 | learning rate : 0.0010\n",
      "step: 172 | accuracy : 0.9075 | learning rate : 0.0010\n",
      "step: 173 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 174 | accuracy : 0.9073 | learning rate : 0.0010\n",
      "step: 175 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 176 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 177 | accuracy : 0.9069 | learning rate : 0.0010\n",
      "step: 178 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 179 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 180 | accuracy : 0.9069 | learning rate : 0.0010\n",
      "step: 181 | accuracy : 0.9052 | learning rate : 0.0010\n",
      "step: 182 | accuracy : 0.9057 | learning rate : 0.0010\n",
      "step: 183 | accuracy : 0.9061 | learning rate : 0.0010\n",
      "step: 184 | accuracy : 0.9064 | learning rate : 0.0010\n",
      "step: 185 | accuracy : 0.9061 | learning rate : 0.0010\n",
      "step: 186 | accuracy : 0.9062 | learning rate : 0.0010\n",
      "step: 187 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 188 | accuracy : 0.9064 | learning rate : 0.0010\n",
      "step: 189 | accuracy : 0.9063 | learning rate : 0.0010\n",
      "step: 190 | accuracy : 0.9061 | learning rate : 0.0010\n",
      "step: 191 | accuracy : 0.9069 | learning rate : 0.0010\n",
      "step: 192 | accuracy : 0.9065 | learning rate : 0.0010\n",
      "step: 193 | accuracy : 0.9074 | learning rate : 0.0010\n",
      "step: 194 | accuracy : 0.9074 | learning rate : 0.0010\n",
      "step: 195 | accuracy : 0.9073 | learning rate : 0.0010\n",
      "step: 196 | accuracy : 0.9076 | learning rate : 0.0010\n",
      "step: 197 | accuracy : 0.9071 | learning rate : 0.0010\n",
      "step: 198 | accuracy : 0.9080 | learning rate : 0.0010\n",
      "step: 199 | accuracy : 0.9082 | learning rate : 0.0010\n",
      "step: 200 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 201 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 202 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 203 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 204 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 205 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 206 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 207 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 208 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 209 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 210 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 211 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 212 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 213 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 214 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 215 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 216 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 217 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 218 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 219 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 220 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 221 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 222 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 223 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 224 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 225 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 226 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 227 | accuracy : 0.9072 | learning rate : 0.0001\n",
      "step: 228 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 229 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 230 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 231 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 232 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 233 | accuracy : 0.9078 | learning rate : 0.0001\n",
      "step: 234 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 235 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 236 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 237 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 238 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 239 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 240 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 241 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 242 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 243 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 244 | accuracy : 0.9073 | learning rate : 0.0001\n",
      "step: 245 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 246 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 247 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 248 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 249 | accuracy : 0.9078 | learning rate : 0.0001\n",
      "step: 250 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 251 | accuracy : 0.9078 | learning rate : 0.0001\n",
      "step: 252 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 253 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 254 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 255 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 256 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 257 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 258 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 259 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 260 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 261 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 262 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 263 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 264 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 265 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 266 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 267 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 268 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 269 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 270 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 271 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 272 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 273 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 274 | accuracy : 0.9074 | learning rate : 0.0001\n",
      "step: 275 | accuracy : 0.9075 | learning rate : 0.0001\n",
      "step: 276 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 277 | accuracy : 0.9076 | learning rate : 0.0001\n",
      "step: 278 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 279 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 280 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 281 | accuracy : 0.9080 | learning rate : 0.0001\n",
      "step: 282 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 283 | accuracy : 0.9078 | learning rate : 0.0001\n",
      "step: 284 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 285 | accuracy : 0.9077 | learning rate : 0.0001\n",
      "step: 286 | accuracy : 0.9080 | learning rate : 0.0001\n",
      "step: 287 | accuracy : 0.9079 | learning rate : 0.0001\n",
      "step: 288 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 289 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 290 | accuracy : 0.9080 | learning rate : 0.0001\n",
      "step: 291 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 292 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 293 | accuracy : 0.9081 | learning rate : 0.0001\n",
      "step: 294 | accuracy : 0.9082 | learning rate : 0.0001\n",
      "step: 295 | accuracy : 0.9082 | learning rate : 0.0001\n",
      "step: 296 | accuracy : 0.9083 | learning rate : 0.0001\n",
      "step: 297 | accuracy : 0.9083 | learning rate : 0.0001\n",
      "step: 298 | accuracy : 0.9085 | learning rate : 0.0001\n",
      "step: 299 | accuracy : 0.9086 | learning rate : 0.0001\n",
      "step: 300 | accuracy : 0.9086 | learning rate : 0.0000\n",
      "step: 301 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 302 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 303 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 304 | accuracy : 0.9086 | learning rate : 0.0000\n",
      "step: 305 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 306 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 307 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 308 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 309 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 310 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 311 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 312 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 313 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 314 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 315 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 316 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 317 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 318 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 319 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 320 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 321 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 322 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 323 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 324 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 325 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 326 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 327 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 328 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 329 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 330 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 331 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 332 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 333 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 334 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 335 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 336 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 337 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 338 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 339 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 340 | accuracy : 0.9085 | learning rate : 0.0000\n",
      "step: 341 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 342 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 343 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 344 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 345 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 346 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 347 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 348 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 349 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 350 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 351 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 352 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 353 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 354 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 355 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 356 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 357 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 358 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 359 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 360 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 361 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 362 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 363 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 364 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 365 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 366 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 367 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 368 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 369 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 370 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 371 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 372 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 373 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 374 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 375 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 376 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 377 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 378 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 379 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 380 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 381 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 382 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 383 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 384 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 385 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 386 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 387 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 388 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 389 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 390 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 391 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 392 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 393 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 394 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 395 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 396 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 397 | accuracy : 0.9082 | learning rate : 0.0000\n",
      "step: 398 | accuracy : 0.9083 | learning rate : 0.0000\n",
      "step: 399 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 400 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 401 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 402 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 403 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 404 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 405 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 406 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 407 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 408 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 409 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 410 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 411 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 412 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 413 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 414 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 415 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 416 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 417 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 418 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 419 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 420 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 421 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 422 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 423 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 424 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 425 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 426 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 427 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 428 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 429 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 430 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 431 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 432 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 433 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 434 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 435 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 436 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 437 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 438 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 439 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 440 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 441 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 442 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 443 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 444 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 445 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 446 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 447 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 448 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 449 | accuracy : 0.9084 | learning rate : 0.0000\n",
      "step: 450 | accuracy : 0.9084 | learning rate : 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d4299c640e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0maccuracy_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step: {:01d} | accuracy : {:.4f} | learning rate : {:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 run_metadata):\n\u001b[1;32m   1116\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1166\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "# set variables\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# placeholder for input data\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# placeholder for learning rate\n",
    "learning_rate = tf.placeholder(\"float\", shape=[])\n",
    "\n",
    "# build softmax model\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# optimization\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#set init learning rate\n",
    "lr = 0.01\n",
    "\n",
    "for step in range(1000):\n",
    "\n",
    "    if(step % 100 == 0 and step != 0):\n",
    "        lr = lr/10\n",
    "\n",
    "    # get 100 images for each iteration\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "    # train the model\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys, learning_rate:lr})\n",
    "\n",
    "    # test teh results\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels})\n",
    "\n",
    "    print('step: {:01d} | accuracy : {:.4f} | learning rate : {:.4f}'.format(step, float(accuracy_), lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "step: 0 | accuracy : 0.2106 | learning rate : 0.0100\n",
      "step: 1 | accuracy : 0.4193 | learning rate : 0.0100\n",
      "step: 2 | accuracy : 0.3233 | learning rate : 0.0100\n",
      "step: 3 | accuracy : 0.5013 | learning rate : 0.0100\n",
      "step: 4 | accuracy : 0.4797 | learning rate : 0.0100\n",
      "step: 5 | accuracy : 0.5484 | learning rate : 0.0100\n",
      "step: 6 | accuracy : 0.5543 | learning rate : 0.0100\n",
      "step: 7 | accuracy : 0.6362 | learning rate : 0.0100\n",
      "step: 8 | accuracy : 0.5875 | learning rate : 0.0100\n",
      "step: 9 | accuracy : 0.6278 | learning rate : 0.0100\n",
      "step: 10 | accuracy : 0.6399 | learning rate : 0.0100\n",
      "step: 11 | accuracy : 0.6881 | learning rate : 0.0100\n",
      "step: 12 | accuracy : 0.7412 | learning rate : 0.0100\n",
      "step: 13 | accuracy : 0.7660 | learning rate : 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a1506fcebe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"float\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0maccuracy_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step: {:01d} | accuracy : {:.4f} | learning rate : {:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                 run_metadata):\n\u001b[1;32m   1116\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32mC:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1166\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "# set variables\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# placeholder for input data\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# placeholder for learning rate\n",
    "learning_rate = tf.placeholder(\"float\", shape=[])\n",
    "\n",
    "# build softmax model\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# optimization\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#set init learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# step decay, exponential decay, 1/t decay\n",
    "for step in range(1000):\n",
    "\n",
    "    if(step % 100 == 0 and step != 0):\n",
    "        lr = lr / 10\n",
    "\n",
    "    # get 100 images for each iteration\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "    # train the model\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys, learning_rate:lr})\n",
    "\n",
    "    # test teh results\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels})\n",
    "\n",
    "    print('step: {:01d} | accuracy : {:.4f} | learning rate : {:.4f}'.format(step, float(accuracy_), lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Batch Size\n",
    "\n",
    " 네트워크를 한번 propagation할 때 데이터 수를 의미\n",
    " batch size = 보통 128개\n",
    " ex. 1050개의 데이터, batch size 100\n",
    "  - 처음 포워드할 때 [1~100], 다음 [101~200], [201~300], ..., [1000~1050]\n",
    "  - 데이터를 한번 다 도는 것을 1 Epoch라 함.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "a_0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "middle = 30\n",
    "w_1 = tf.Variable(tf.truncated_normal([784, middle]))\n",
    "b_1 = tf.Variable(tf.truncated_normal([1, middle]))\n",
    "w_2 = tf.Variable(tf.truncated_normal([middle, 10]))\n",
    "b_2 = tf.Variable(tf.truncated_normal([1, 10]))\n",
    "\n",
    "def sigma(x):\n",
    "    return tf.div(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n",
    "\n",
    "\n",
    "z_1 = tf.add(tf.matmul(a_0, w_1), b_1)\n",
    "a_1 = sigma(z_1)\n",
    "z_2 = tf.add(tf.matmul(a_1, w_2), b_2)\n",
    "a_2 = sigma(z_2)\n",
    "\n",
    "diff = tf.subtract(a_2, y)\n",
    "d_z_2 = tf.multiply(diff, sigmaprime(z_2))\n",
    "d_b_2 = d_z_2\n",
    "d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\n",
    "\n",
    "d_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))\n",
    "d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n",
    "d_b_1 = d_z_1\n",
    "d_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)\n",
    "\n",
    "eta = tf.constant(0.5)\n",
    "step = [\n",
    "    tf.assign(w_1,\n",
    "            tf.subtract(w_1, tf.multiply(eta, d_w_1)))\n",
    "  , tf.assign(b_1,\n",
    "            tf.subtract(b_1, tf.multiply(eta,\n",
    "                               tf.reduce_mean(d_b_1, axis=[0]))))\n",
    "  , tf.assign(w_2,\n",
    "            tf.subtract(w_2, tf.multiply(eta, d_w_2)))\n",
    "  , tf.assign(b_2,\n",
    "            tf.subtract(b_2, tf.multiply(eta,\n",
    "                               tf.reduce_mean(d_b_2, axis=[0]))))\n",
    "]\n",
    "\n",
    "acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in xrange(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(10)\n",
    "    sess.run(step, feed_dict = {a_0: batch_xs,\n",
    "                                y : batch_ys})\n",
    "    if i % 1000 == 0:\n",
    "        res = sess.run(acct_res, feed_dict =\n",
    "                       {a_0: mnist.test.images[:1000],\n",
    "                        y : mnist.test.labels[:1000]})\n",
    "        print res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "Epoch: 0001 cost= 166.349192429\n",
      "Epoch: 0002 cost= 38.444730281\n",
      "Epoch: 0003 cost= 24.216119050\n",
      "Epoch: 0004 cost= 16.946142122\n",
      "Epoch: 0005 cost= 12.332160136\n",
      "Epoch: 0006 cost= 9.256018394\n",
      "Epoch: 0007 cost= 7.025145741\n",
      "Epoch: 0008 cost= 5.209423389\n",
      "Epoch: 0009 cost= 4.002566180\n",
      "Epoch: 0010 cost= 2.987386289\n",
      "Epoch: 0011 cost= 2.248583235\n",
      "Epoch: 0012 cost= 1.697362977\n",
      "Epoch: 0013 cost= 1.322869226\n",
      "Epoch: 0014 cost= 1.061509206\n",
      "Epoch: 0015 cost= 0.810548451\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9457\n"
     ]
    }
   ],
   "source": [
    "# A Multilayer Perceptron implementation example using TensorFlow\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "add = tf.add(x, y)\n",
    "mul = tf.multiply(x, y)\n",
    "\n",
    "add_hist = tf.summary.scalar(\"add_scalar\", add)\n",
    "mul_hist = tf.summary.scalar(\"mul_scalar\", mul)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./sample\", sess.graph)\n",
    "    \n",
    "    for step in range(100):\n",
    "        summary = sess.run(merged, feed_dict={x: step * 1.0, y : 2.0})\n",
    "        writer.add_summary(summary, step)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "WARNING:tensorflow:From C:\\Users\\SEC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step: 0 | accuracy : 0.3478\n",
      "step: 1 | accuracy : 0.2854\n",
      "step: 2 | accuracy : 0.3315\n",
      "step: 3 | accuracy : 0.5152\n",
      "step: 4 | accuracy : 0.4529\n",
      "step: 5 | accuracy : 0.5188\n",
      "step: 6 | accuracy : 0.5647\n",
      "step: 7 | accuracy : 0.6730\n",
      "step: 8 | accuracy : 0.7221\n",
      "step: 9 | accuracy : 0.7390\n",
      "step: 10 | accuracy : 0.6830\n",
      "step: 11 | accuracy : 0.7413\n",
      "step: 12 | accuracy : 0.6395\n",
      "step: 13 | accuracy : 0.6661\n",
      "step: 14 | accuracy : 0.7462\n",
      "step: 15 | accuracy : 0.7428\n",
      "step: 16 | accuracy : 0.6805\n",
      "step: 17 | accuracy : 0.7737\n",
      "step: 18 | accuracy : 0.8471\n",
      "step: 19 | accuracy : 0.7987\n",
      "step: 20 | accuracy : 0.8411\n",
      "step: 21 | accuracy : 0.8590\n",
      "step: 22 | accuracy : 0.8458\n",
      "step: 23 | accuracy : 0.8425\n",
      "step: 24 | accuracy : 0.8430\n",
      "step: 25 | accuracy : 0.8504\n",
      "step: 26 | accuracy : 0.8548\n",
      "step: 27 | accuracy : 0.8468\n",
      "step: 28 | accuracy : 0.8452\n",
      "step: 29 | accuracy : 0.8236\n",
      "step: 30 | accuracy : 0.7908\n",
      "step: 31 | accuracy : 0.8218\n",
      "step: 32 | accuracy : 0.8167\n",
      "step: 33 | accuracy : 0.7676\n",
      "step: 34 | accuracy : 0.8367\n",
      "step: 35 | accuracy : 0.8401\n",
      "step: 36 | accuracy : 0.8628\n",
      "step: 37 | accuracy : 0.8795\n",
      "step: 38 | accuracy : 0.8711\n",
      "step: 39 | accuracy : 0.8674\n",
      "step: 40 | accuracy : 0.8374\n",
      "step: 41 | accuracy : 0.8280\n",
      "step: 42 | accuracy : 0.8789\n",
      "step: 43 | accuracy : 0.8661\n",
      "step: 44 | accuracy : 0.8748\n",
      "step: 45 | accuracy : 0.8235\n",
      "step: 46 | accuracy : 0.8159\n",
      "step: 47 | accuracy : 0.8775\n",
      "step: 48 | accuracy : 0.8549\n",
      "step: 49 | accuracy : 0.8243\n",
      "step: 50 | accuracy : 0.8372\n",
      "step: 51 | accuracy : 0.8742\n",
      "step: 52 | accuracy : 0.8809\n",
      "step: 53 | accuracy : 0.8721\n",
      "step: 54 | accuracy : 0.8703\n",
      "step: 55 | accuracy : 0.8582\n",
      "step: 56 | accuracy : 0.8867\n",
      "step: 57 | accuracy : 0.8672\n",
      "step: 58 | accuracy : 0.8845\n",
      "step: 59 | accuracy : 0.8921\n",
      "step: 60 | accuracy : 0.8581\n",
      "step: 61 | accuracy : 0.8176\n",
      "step: 62 | accuracy : 0.8580\n",
      "step: 63 | accuracy : 0.8789\n",
      "step: 64 | accuracy : 0.8817\n",
      "step: 65 | accuracy : 0.8855\n",
      "step: 66 | accuracy : 0.8783\n",
      "step: 67 | accuracy : 0.8821\n",
      "step: 68 | accuracy : 0.8835\n",
      "step: 69 | accuracy : 0.8906\n",
      "step: 70 | accuracy : 0.8959\n",
      "step: 71 | accuracy : 0.8846\n",
      "step: 72 | accuracy : 0.8819\n",
      "step: 73 | accuracy : 0.8863\n",
      "step: 74 | accuracy : 0.8646\n",
      "step: 75 | accuracy : 0.8837\n",
      "step: 76 | accuracy : 0.8876\n",
      "step: 77 | accuracy : 0.8328\n",
      "step: 78 | accuracy : 0.8628\n",
      "step: 79 | accuracy : 0.8513\n",
      "step: 80 | accuracy : 0.8594\n",
      "step: 81 | accuracy : 0.8406\n",
      "step: 82 | accuracy : 0.8768\n",
      "step: 83 | accuracy : 0.8821\n",
      "step: 84 | accuracy : 0.8518\n",
      "step: 85 | accuracy : 0.8723\n",
      "step: 86 | accuracy : 0.8781\n",
      "step: 87 | accuracy : 0.8806\n",
      "step: 88 | accuracy : 0.8878\n",
      "step: 89 | accuracy : 0.8705\n",
      "step: 90 | accuracy : 0.8889\n",
      "step: 91 | accuracy : 0.8815\n",
      "step: 92 | accuracy : 0.8874\n",
      "step: 93 | accuracy : 0.8862\n",
      "step: 94 | accuracy : 0.8896\n",
      "step: 95 | accuracy : 0.8753\n",
      "step: 96 | accuracy : 0.8786\n",
      "step: 97 | accuracy : 0.9000\n",
      "step: 98 | accuracy : 0.8733\n",
      "step: 99 | accuracy : 0.8922\n",
      "step: 100 | accuracy : 0.8589\n",
      "step: 101 | accuracy : 0.8921\n",
      "step: 102 | accuracy : 0.8866\n",
      "step: 103 | accuracy : 0.9007\n",
      "step: 104 | accuracy : 0.8914\n",
      "step: 105 | accuracy : 0.8824\n",
      "step: 106 | accuracy : 0.8933\n",
      "step: 107 | accuracy : 0.9016\n",
      "step: 108 | accuracy : 0.8984\n",
      "step: 109 | accuracy : 0.8957\n",
      "step: 110 | accuracy : 0.8899\n",
      "step: 111 | accuracy : 0.8954\n",
      "step: 112 | accuracy : 0.8953\n",
      "step: 113 | accuracy : 0.8933\n",
      "step: 114 | accuracy : 0.8979\n",
      "step: 115 | accuracy : 0.9051\n",
      "step: 116 | accuracy : 0.8937\n",
      "step: 117 | accuracy : 0.8979\n",
      "step: 118 | accuracy : 0.8972\n",
      "step: 119 | accuracy : 0.8918\n",
      "step: 120 | accuracy : 0.8947\n",
      "step: 121 | accuracy : 0.8728\n",
      "step: 122 | accuracy : 0.8900\n",
      "step: 123 | accuracy : 0.8819\n",
      "step: 124 | accuracy : 0.8824\n",
      "step: 125 | accuracy : 0.8971\n",
      "step: 126 | accuracy : 0.9053\n",
      "step: 127 | accuracy : 0.8932\n",
      "step: 128 | accuracy : 0.8772\n",
      "step: 129 | accuracy : 0.8867\n",
      "step: 130 | accuracy : 0.8751\n",
      "step: 131 | accuracy : 0.9055\n",
      "step: 132 | accuracy : 0.8986\n",
      "step: 133 | accuracy : 0.8937\n",
      "step: 134 | accuracy : 0.9048\n",
      "step: 135 | accuracy : 0.9044\n",
      "step: 136 | accuracy : 0.8943\n",
      "step: 137 | accuracy : 0.8898\n",
      "step: 138 | accuracy : 0.8942\n",
      "step: 139 | accuracy : 0.8760\n",
      "step: 140 | accuracy : 0.8915\n",
      "step: 141 | accuracy : 0.8976\n",
      "step: 142 | accuracy : 0.9009\n",
      "step: 143 | accuracy : 0.8990\n",
      "step: 144 | accuracy : 0.9053\n",
      "step: 145 | accuracy : 0.8693\n",
      "step: 146 | accuracy : 0.8942\n",
      "step: 147 | accuracy : 0.8959\n",
      "step: 148 | accuracy : 0.8753\n",
      "step: 149 | accuracy : 0.8903\n",
      "step: 150 | accuracy : 0.9008\n",
      "step: 151 | accuracy : 0.9081\n",
      "step: 152 | accuracy : 0.9086\n",
      "step: 153 | accuracy : 0.9041\n",
      "step: 154 | accuracy : 0.9020\n",
      "step: 155 | accuracy : 0.9014\n",
      "step: 156 | accuracy : 0.9007\n",
      "step: 157 | accuracy : 0.8895\n",
      "step: 158 | accuracy : 0.9029\n",
      "step: 159 | accuracy : 0.8981\n",
      "step: 160 | accuracy : 0.8998\n",
      "step: 161 | accuracy : 0.9048\n",
      "step: 162 | accuracy : 0.9069\n",
      "step: 163 | accuracy : 0.9028\n",
      "step: 164 | accuracy : 0.9078\n",
      "step: 165 | accuracy : 0.9095\n",
      "step: 166 | accuracy : 0.9054\n",
      "step: 167 | accuracy : 0.9087\n",
      "step: 168 | accuracy : 0.9050\n",
      "step: 169 | accuracy : 0.9066\n",
      "step: 170 | accuracy : 0.9039\n",
      "step: 171 | accuracy : 0.8985\n",
      "step: 172 | accuracy : 0.9010\n",
      "step: 173 | accuracy : 0.8847\n",
      "step: 174 | accuracy : 0.9023\n",
      "step: 175 | accuracy : 0.8988\n",
      "step: 176 | accuracy : 0.8914\n",
      "step: 177 | accuracy : 0.9026\n",
      "step: 178 | accuracy : 0.8826\n",
      "step: 179 | accuracy : 0.8990\n",
      "step: 180 | accuracy : 0.9064\n",
      "step: 181 | accuracy : 0.9094\n",
      "step: 182 | accuracy : 0.8953\n",
      "step: 183 | accuracy : 0.8721\n",
      "step: 184 | accuracy : 0.8699\n",
      "step: 185 | accuracy : 0.8924\n",
      "step: 186 | accuracy : 0.9066\n",
      "step: 187 | accuracy : 0.9003\n",
      "step: 188 | accuracy : 0.9070\n",
      "step: 189 | accuracy : 0.8799\n",
      "step: 190 | accuracy : 0.8869\n",
      "step: 191 | accuracy : 0.9085\n",
      "step: 192 | accuracy : 0.9095\n",
      "step: 193 | accuracy : 0.9050\n",
      "step: 194 | accuracy : 0.9029\n",
      "step: 195 | accuracy : 0.9063\n",
      "step: 196 | accuracy : 0.9107\n",
      "step: 197 | accuracy : 0.9070\n",
      "step: 198 | accuracy : 0.8953\n",
      "step: 199 | accuracy : 0.8940\n",
      "step: 200 | accuracy : 0.9055\n",
      "step: 201 | accuracy : 0.9055\n",
      "step: 202 | accuracy : 0.9077\n",
      "step: 203 | accuracy : 0.9065\n",
      "step: 204 | accuracy : 0.8987\n",
      "step: 205 | accuracy : 0.9026\n",
      "step: 206 | accuracy : 0.8955\n",
      "step: 207 | accuracy : 0.8864\n",
      "step: 208 | accuracy : 0.8894\n",
      "step: 209 | accuracy : 0.8887\n",
      "step: 210 | accuracy : 0.9009\n",
      "step: 211 | accuracy : 0.8976\n",
      "step: 212 | accuracy : 0.8987\n",
      "step: 213 | accuracy : 0.9062\n",
      "step: 214 | accuracy : 0.8950\n",
      "step: 215 | accuracy : 0.8823\n",
      "step: 216 | accuracy : 0.8941\n",
      "step: 217 | accuracy : 0.8760\n",
      "step: 218 | accuracy : 0.8904\n",
      "step: 219 | accuracy : 0.9026\n",
      "step: 220 | accuracy : 0.9056\n",
      "step: 221 | accuracy : 0.9035\n",
      "step: 222 | accuracy : 0.9024\n",
      "step: 223 | accuracy : 0.9032\n",
      "step: 224 | accuracy : 0.9112\n",
      "step: 225 | accuracy : 0.9101\n",
      "step: 226 | accuracy : 0.8912\n",
      "step: 227 | accuracy : 0.8985\n",
      "step: 228 | accuracy : 0.9064\n",
      "step: 229 | accuracy : 0.9056\n",
      "step: 230 | accuracy : 0.8933\n",
      "step: 231 | accuracy : 0.9036\n",
      "step: 232 | accuracy : 0.9086\n",
      "step: 233 | accuracy : 0.8999\n",
      "step: 234 | accuracy : 0.9025\n",
      "step: 235 | accuracy : 0.9066\n",
      "step: 236 | accuracy : 0.8866\n",
      "step: 237 | accuracy : 0.8888\n",
      "step: 238 | accuracy : 0.8810\n",
      "step: 239 | accuracy : 0.9021\n",
      "step: 240 | accuracy : 0.9099\n",
      "step: 241 | accuracy : 0.9134\n",
      "step: 242 | accuracy : 0.9016\n",
      "step: 243 | accuracy : 0.9086\n",
      "step: 244 | accuracy : 0.9043\n",
      "step: 245 | accuracy : 0.9088\n",
      "step: 246 | accuracy : 0.9050\n",
      "step: 247 | accuracy : 0.8971\n",
      "step: 248 | accuracy : 0.9036\n",
      "step: 249 | accuracy : 0.9007\n",
      "step: 250 | accuracy : 0.8889\n",
      "step: 251 | accuracy : 0.9008\n",
      "step: 252 | accuracy : 0.9105\n",
      "step: 253 | accuracy : 0.9000\n",
      "step: 254 | accuracy : 0.9109\n",
      "step: 255 | accuracy : 0.8975\n",
      "step: 256 | accuracy : 0.9057\n",
      "step: 257 | accuracy : 0.8970\n",
      "step: 258 | accuracy : 0.8869\n",
      "step: 259 | accuracy : 0.9053\n",
      "step: 260 | accuracy : 0.9018\n",
      "step: 261 | accuracy : 0.9085\n",
      "step: 262 | accuracy : 0.8985\n",
      "step: 263 | accuracy : 0.9064\n",
      "step: 264 | accuracy : 0.8996\n",
      "step: 265 | accuracy : 0.9083\n",
      "step: 266 | accuracy : 0.9018\n",
      "step: 267 | accuracy : 0.9070\n",
      "step: 268 | accuracy : 0.8648\n",
      "step: 269 | accuracy : 0.8709\n",
      "step: 270 | accuracy : 0.8944\n",
      "step: 271 | accuracy : 0.9037\n",
      "step: 272 | accuracy : 0.8922\n",
      "step: 273 | accuracy : 0.9088\n",
      "step: 274 | accuracy : 0.9076\n",
      "step: 275 | accuracy : 0.9057\n",
      "step: 276 | accuracy : 0.9097\n",
      "step: 277 | accuracy : 0.9054\n",
      "step: 278 | accuracy : 0.9120\n",
      "step: 279 | accuracy : 0.9012\n",
      "step: 280 | accuracy : 0.9036\n",
      "step: 281 | accuracy : 0.9061\n",
      "step: 282 | accuracy : 0.9071\n",
      "step: 283 | accuracy : 0.9072\n",
      "step: 284 | accuracy : 0.9025\n",
      "step: 285 | accuracy : 0.9075\n",
      "step: 286 | accuracy : 0.9037\n",
      "step: 287 | accuracy : 0.9029\n",
      "step: 288 | accuracy : 0.9045\n",
      "step: 289 | accuracy : 0.9097\n",
      "step: 290 | accuracy : 0.9078\n",
      "step: 291 | accuracy : 0.9035\n",
      "step: 292 | accuracy : 0.9081\n",
      "step: 293 | accuracy : 0.9091\n",
      "step: 294 | accuracy : 0.9094\n",
      "step: 295 | accuracy : 0.9104\n",
      "step: 296 | accuracy : 0.9088\n",
      "step: 297 | accuracy : 0.9141\n",
      "step: 298 | accuracy : 0.9133\n",
      "step: 299 | accuracy : 0.9068\n",
      "step: 300 | accuracy : 0.9098\n",
      "step: 301 | accuracy : 0.9121\n",
      "step: 302 | accuracy : 0.9065\n",
      "step: 303 | accuracy : 0.8837\n",
      "step: 304 | accuracy : 0.9015\n",
      "step: 305 | accuracy : 0.8891\n",
      "step: 306 | accuracy : 0.8979\n",
      "step: 307 | accuracy : 0.9077\n",
      "step: 308 | accuracy : 0.9106\n",
      "step: 309 | accuracy : 0.9002\n",
      "step: 310 | accuracy : 0.8937\n",
      "step: 311 | accuracy : 0.8811\n",
      "step: 312 | accuracy : 0.9114\n",
      "step: 313 | accuracy : 0.9057\n",
      "step: 314 | accuracy : 0.9044\n",
      "step: 315 | accuracy : 0.9131\n",
      "step: 316 | accuracy : 0.9063\n",
      "step: 317 | accuracy : 0.9113\n",
      "step: 318 | accuracy : 0.9097\n",
      "step: 319 | accuracy : 0.9015\n",
      "step: 320 | accuracy : 0.8894\n",
      "step: 321 | accuracy : 0.8916\n",
      "step: 322 | accuracy : 0.9127\n",
      "step: 323 | accuracy : 0.9102\n",
      "step: 324 | accuracy : 0.9080\n",
      "step: 325 | accuracy : 0.9110\n",
      "step: 326 | accuracy : 0.9070\n",
      "step: 327 | accuracy : 0.9134\n",
      "step: 328 | accuracy : 0.8992\n",
      "step: 329 | accuracy : 0.8891\n",
      "step: 330 | accuracy : 0.9063\n",
      "step: 331 | accuracy : 0.9154\n",
      "step: 332 | accuracy : 0.9106\n",
      "step: 333 | accuracy : 0.9142\n",
      "step: 334 | accuracy : 0.9038\n",
      "step: 335 | accuracy : 0.8939\n",
      "step: 336 | accuracy : 0.9093\n",
      "step: 337 | accuracy : 0.9120\n",
      "step: 338 | accuracy : 0.9030\n",
      "step: 339 | accuracy : 0.9142\n",
      "step: 340 | accuracy : 0.9095\n",
      "step: 341 | accuracy : 0.9135\n",
      "step: 342 | accuracy : 0.9136\n",
      "step: 343 | accuracy : 0.8922\n",
      "step: 344 | accuracy : 0.9009\n",
      "step: 345 | accuracy : 0.9072\n",
      "step: 346 | accuracy : 0.9036\n",
      "step: 347 | accuracy : 0.9080\n",
      "step: 348 | accuracy : 0.9044\n",
      "step: 349 | accuracy : 0.9114\n",
      "step: 350 | accuracy : 0.9051\n",
      "step: 351 | accuracy : 0.9087\n",
      "step: 352 | accuracy : 0.9045\n",
      "step: 353 | accuracy : 0.8993\n",
      "step: 354 | accuracy : 0.9092\n",
      "step: 355 | accuracy : 0.9023\n",
      "step: 356 | accuracy : 0.9017\n",
      "step: 357 | accuracy : 0.9098\n",
      "step: 358 | accuracy : 0.9155\n",
      "step: 359 | accuracy : 0.9128\n",
      "step: 360 | accuracy : 0.9105\n",
      "step: 361 | accuracy : 0.9099\n",
      "step: 362 | accuracy : 0.9124\n",
      "step: 363 | accuracy : 0.9022\n",
      "step: 364 | accuracy : 0.9099\n",
      "step: 365 | accuracy : 0.9086\n",
      "step: 366 | accuracy : 0.9128\n",
      "step: 367 | accuracy : 0.9119\n",
      "step: 368 | accuracy : 0.9055\n",
      "step: 369 | accuracy : 0.9068\n",
      "step: 370 | accuracy : 0.8906\n",
      "step: 371 | accuracy : 0.9105\n",
      "step: 372 | accuracy : 0.9057\n",
      "step: 373 | accuracy : 0.9115\n",
      "step: 374 | accuracy : 0.8858\n",
      "step: 375 | accuracy : 0.8650\n",
      "step: 376 | accuracy : 0.8840\n",
      "step: 377 | accuracy : 0.9071\n",
      "step: 378 | accuracy : 0.9012\n",
      "step: 379 | accuracy : 0.9074\n",
      "step: 380 | accuracy : 0.9100\n",
      "step: 381 | accuracy : 0.9164\n",
      "step: 382 | accuracy : 0.9110\n",
      "step: 383 | accuracy : 0.9065\n",
      "step: 384 | accuracy : 0.9102\n",
      "step: 385 | accuracy : 0.9067\n",
      "step: 386 | accuracy : 0.9086\n",
      "step: 387 | accuracy : 0.9080\n",
      "step: 388 | accuracy : 0.9095\n",
      "step: 389 | accuracy : 0.8939\n",
      "step: 390 | accuracy : 0.8431\n",
      "step: 391 | accuracy : 0.8931\n",
      "step: 392 | accuracy : 0.9074\n",
      "step: 393 | accuracy : 0.9069\n",
      "step: 394 | accuracy : 0.9016\n",
      "step: 395 | accuracy : 0.9160\n",
      "step: 396 | accuracy : 0.9091\n",
      "step: 397 | accuracy : 0.9129\n",
      "step: 398 | accuracy : 0.8929\n",
      "step: 399 | accuracy : 0.9132\n",
      "step: 400 | accuracy : 0.8994\n",
      "step: 401 | accuracy : 0.8992\n",
      "step: 402 | accuracy : 0.8986\n",
      "step: 403 | accuracy : 0.9114\n",
      "step: 404 | accuracy : 0.9131\n",
      "step: 405 | accuracy : 0.9068\n",
      "step: 406 | accuracy : 0.9075\n",
      "step: 407 | accuracy : 0.9077\n",
      "step: 408 | accuracy : 0.9031\n",
      "step: 409 | accuracy : 0.9095\n",
      "step: 410 | accuracy : 0.9081\n",
      "step: 411 | accuracy : 0.9039\n",
      "step: 412 | accuracy : 0.9083\n",
      "step: 413 | accuracy : 0.9114\n",
      "step: 414 | accuracy : 0.9100\n",
      "step: 415 | accuracy : 0.9105\n",
      "step: 416 | accuracy : 0.9058\n",
      "step: 417 | accuracy : 0.9122\n",
      "step: 418 | accuracy : 0.9047\n",
      "step: 419 | accuracy : 0.9101\n",
      "step: 420 | accuracy : 0.9097\n",
      "step: 421 | accuracy : 0.9142\n",
      "step: 422 | accuracy : 0.9063\n",
      "step: 423 | accuracy : 0.8945\n",
      "step: 424 | accuracy : 0.9123\n",
      "step: 425 | accuracy : 0.9070\n",
      "step: 426 | accuracy : 0.9080\n",
      "step: 427 | accuracy : 0.9030\n",
      "step: 428 | accuracy : 0.9135\n",
      "step: 429 | accuracy : 0.9064\n",
      "step: 430 | accuracy : 0.9052\n",
      "step: 431 | accuracy : 0.8903\n",
      "step: 432 | accuracy : 0.9162\n",
      "step: 433 | accuracy : 0.9051\n",
      "step: 434 | accuracy : 0.9131\n",
      "step: 435 | accuracy : 0.9054\n",
      "step: 436 | accuracy : 0.8944\n",
      "step: 437 | accuracy : 0.9083\n",
      "step: 438 | accuracy : 0.8764\n",
      "step: 439 | accuracy : 0.9011\n",
      "step: 440 | accuracy : 0.9130\n",
      "step: 441 | accuracy : 0.9138\n",
      "step: 442 | accuracy : 0.9140\n",
      "step: 443 | accuracy : 0.9083\n",
      "step: 444 | accuracy : 0.9157\n",
      "step: 445 | accuracy : 0.9027\n",
      "step: 446 | accuracy : 0.9163\n",
      "step: 447 | accuracy : 0.9131\n",
      "step: 448 | accuracy : 0.9140\n",
      "step: 449 | accuracy : 0.8927\n",
      "step: 450 | accuracy : 0.9039\n",
      "step: 451 | accuracy : 0.9056\n",
      "step: 452 | accuracy : 0.9153\n",
      "step: 453 | accuracy : 0.9131\n",
      "step: 454 | accuracy : 0.9146\n",
      "step: 455 | accuracy : 0.9143\n",
      "step: 456 | accuracy : 0.9030\n",
      "step: 457 | accuracy : 0.9149\n",
      "step: 458 | accuracy : 0.9046\n",
      "step: 459 | accuracy : 0.9040\n",
      "step: 460 | accuracy : 0.9115\n",
      "step: 461 | accuracy : 0.9039\n",
      "step: 462 | accuracy : 0.9136\n",
      "step: 463 | accuracy : 0.9145\n",
      "step: 464 | accuracy : 0.9135\n",
      "step: 465 | accuracy : 0.9088\n",
      "step: 466 | accuracy : 0.9132\n",
      "step: 467 | accuracy : 0.9028\n",
      "step: 468 | accuracy : 0.9015\n",
      "step: 469 | accuracy : 0.8699\n",
      "step: 470 | accuracy : 0.8948\n",
      "step: 471 | accuracy : 0.9098\n",
      "step: 472 | accuracy : 0.9112\n",
      "step: 473 | accuracy : 0.9067\n",
      "step: 474 | accuracy : 0.9150\n",
      "step: 475 | accuracy : 0.9146\n",
      "step: 476 | accuracy : 0.9081\n",
      "step: 477 | accuracy : 0.9026\n",
      "step: 478 | accuracy : 0.9150\n",
      "step: 479 | accuracy : 0.9116\n",
      "step: 480 | accuracy : 0.9046\n",
      "step: 481 | accuracy : 0.9056\n",
      "step: 482 | accuracy : 0.9074\n",
      "step: 483 | accuracy : 0.9155\n",
      "step: 484 | accuracy : 0.9134\n",
      "step: 485 | accuracy : 0.9109\n",
      "step: 486 | accuracy : 0.9100\n",
      "step: 487 | accuracy : 0.9083\n",
      "step: 488 | accuracy : 0.9065\n",
      "step: 489 | accuracy : 0.9123\n",
      "step: 490 | accuracy : 0.9121\n",
      "step: 491 | accuracy : 0.9101\n",
      "step: 492 | accuracy : 0.9135\n",
      "step: 493 | accuracy : 0.9119\n",
      "step: 494 | accuracy : 0.9124\n",
      "step: 495 | accuracy : 0.8962\n",
      "step: 496 | accuracy : 0.9025\n",
      "step: 497 | accuracy : 0.9137\n",
      "step: 498 | accuracy : 0.9013\n",
      "step: 499 | accuracy : 0.9103\n",
      "step: 500 | accuracy : 0.8992\n",
      "step: 501 | accuracy : 0.9128\n",
      "step: 502 | accuracy : 0.8908\n",
      "step: 503 | accuracy : 0.9050\n",
      "step: 504 | accuracy : 0.9136\n",
      "step: 505 | accuracy : 0.9121\n",
      "step: 506 | accuracy : 0.9082\n",
      "step: 507 | accuracy : 0.9118\n",
      "step: 508 | accuracy : 0.9033\n",
      "step: 509 | accuracy : 0.9141\n",
      "step: 510 | accuracy : 0.9095\n",
      "step: 511 | accuracy : 0.9093\n",
      "step: 512 | accuracy : 0.9069\n",
      "step: 513 | accuracy : 0.9118\n",
      "step: 514 | accuracy : 0.9109\n",
      "step: 515 | accuracy : 0.9130\n",
      "step: 516 | accuracy : 0.8970\n",
      "step: 517 | accuracy : 0.9078\n",
      "step: 518 | accuracy : 0.9100\n",
      "step: 519 | accuracy : 0.9125\n",
      "step: 520 | accuracy : 0.9106\n",
      "step: 521 | accuracy : 0.9037\n",
      "step: 522 | accuracy : 0.9144\n",
      "step: 523 | accuracy : 0.9111\n",
      "step: 524 | accuracy : 0.9147\n",
      "step: 525 | accuracy : 0.9068\n",
      "step: 526 | accuracy : 0.9004\n",
      "step: 527 | accuracy : 0.9159\n",
      "step: 528 | accuracy : 0.9129\n",
      "step: 529 | accuracy : 0.9111\n",
      "step: 530 | accuracy : 0.9050\n",
      "step: 531 | accuracy : 0.9161\n",
      "step: 532 | accuracy : 0.9048\n",
      "step: 533 | accuracy : 0.9088\n",
      "step: 534 | accuracy : 0.9039\n",
      "step: 535 | accuracy : 0.9134\n",
      "step: 536 | accuracy : 0.9145\n",
      "step: 537 | accuracy : 0.9014\n",
      "step: 538 | accuracy : 0.9106\n",
      "step: 539 | accuracy : 0.9176\n",
      "step: 540 | accuracy : 0.9104\n",
      "step: 541 | accuracy : 0.8970\n",
      "step: 542 | accuracy : 0.8956\n",
      "step: 543 | accuracy : 0.8980\n",
      "step: 544 | accuracy : 0.9133\n",
      "step: 545 | accuracy : 0.9122\n",
      "step: 546 | accuracy : 0.9035\n",
      "step: 547 | accuracy : 0.8984\n",
      "step: 548 | accuracy : 0.9004\n",
      "step: 549 | accuracy : 0.9152\n",
      "step: 550 | accuracy : 0.8991\n",
      "step: 551 | accuracy : 0.9051\n",
      "step: 552 | accuracy : 0.9110\n",
      "step: 553 | accuracy : 0.9142\n",
      "step: 554 | accuracy : 0.9021\n",
      "step: 555 | accuracy : 0.9176\n",
      "step: 556 | accuracy : 0.9160\n",
      "step: 557 | accuracy : 0.9038\n",
      "step: 558 | accuracy : 0.9122\n",
      "step: 559 | accuracy : 0.9166\n",
      "step: 560 | accuracy : 0.9122\n",
      "step: 561 | accuracy : 0.9152\n",
      "step: 562 | accuracy : 0.9158\n",
      "step: 563 | accuracy : 0.9109\n",
      "step: 564 | accuracy : 0.9037\n",
      "step: 565 | accuracy : 0.9153\n",
      "step: 566 | accuracy : 0.9053\n",
      "step: 567 | accuracy : 0.9142\n",
      "step: 568 | accuracy : 0.9076\n",
      "step: 569 | accuracy : 0.9110\n",
      "step: 570 | accuracy : 0.9097\n",
      "step: 571 | accuracy : 0.8990\n",
      "step: 572 | accuracy : 0.8947\n",
      "step: 573 | accuracy : 0.9028\n",
      "step: 574 | accuracy : 0.8891\n",
      "step: 575 | accuracy : 0.9066\n",
      "step: 576 | accuracy : 0.9163\n",
      "step: 577 | accuracy : 0.9140\n",
      "step: 578 | accuracy : 0.9108\n",
      "step: 579 | accuracy : 0.9135\n",
      "step: 580 | accuracy : 0.9027\n",
      "step: 581 | accuracy : 0.9043\n",
      "step: 582 | accuracy : 0.9074\n",
      "step: 583 | accuracy : 0.9105\n",
      "step: 584 | accuracy : 0.9099\n",
      "step: 585 | accuracy : 0.9108\n",
      "step: 586 | accuracy : 0.9121\n",
      "step: 587 | accuracy : 0.9129\n",
      "step: 588 | accuracy : 0.9152\n",
      "step: 589 | accuracy : 0.9167\n",
      "step: 590 | accuracy : 0.9079\n",
      "step: 591 | accuracy : 0.9105\n",
      "step: 592 | accuracy : 0.9155\n",
      "step: 593 | accuracy : 0.9119\n",
      "step: 594 | accuracy : 0.9114\n",
      "step: 595 | accuracy : 0.9176\n",
      "step: 596 | accuracy : 0.9121\n",
      "step: 597 | accuracy : 0.9134\n",
      "step: 598 | accuracy : 0.9150\n",
      "step: 599 | accuracy : 0.9133\n",
      "step: 600 | accuracy : 0.9160\n",
      "step: 601 | accuracy : 0.9071\n",
      "step: 602 | accuracy : 0.9092\n",
      "step: 603 | accuracy : 0.9114\n",
      "step: 604 | accuracy : 0.9128\n",
      "step: 605 | accuracy : 0.9167\n",
      "step: 606 | accuracy : 0.9134\n",
      "step: 607 | accuracy : 0.9061\n",
      "step: 608 | accuracy : 0.9048\n",
      "step: 609 | accuracy : 0.9137\n",
      "step: 610 | accuracy : 0.9156\n",
      "step: 611 | accuracy : 0.9136\n",
      "step: 612 | accuracy : 0.9145\n",
      "step: 613 | accuracy : 0.9154\n",
      "step: 614 | accuracy : 0.9132\n",
      "step: 615 | accuracy : 0.9112\n",
      "step: 616 | accuracy : 0.9110\n",
      "step: 617 | accuracy : 0.9157\n",
      "step: 618 | accuracy : 0.9196\n",
      "step: 619 | accuracy : 0.9137\n",
      "step: 620 | accuracy : 0.9176\n",
      "step: 621 | accuracy : 0.9127\n",
      "step: 622 | accuracy : 0.9116\n",
      "step: 623 | accuracy : 0.9027\n",
      "step: 624 | accuracy : 0.9120\n",
      "step: 625 | accuracy : 0.9077\n",
      "step: 626 | accuracy : 0.9166\n",
      "step: 627 | accuracy : 0.9170\n",
      "step: 628 | accuracy : 0.9105\n",
      "step: 629 | accuracy : 0.9113\n",
      "step: 630 | accuracy : 0.9060\n",
      "step: 631 | accuracy : 0.9136\n",
      "step: 632 | accuracy : 0.9071\n",
      "step: 633 | accuracy : 0.9114\n",
      "step: 634 | accuracy : 0.9037\n",
      "step: 635 | accuracy : 0.9133\n",
      "step: 636 | accuracy : 0.9065\n",
      "step: 637 | accuracy : 0.9173\n",
      "step: 638 | accuracy : 0.9117\n",
      "step: 639 | accuracy : 0.9124\n",
      "step: 640 | accuracy : 0.9045\n",
      "step: 641 | accuracy : 0.9051\n",
      "step: 642 | accuracy : 0.9137\n",
      "step: 643 | accuracy : 0.9007\n",
      "step: 644 | accuracy : 0.9171\n",
      "step: 645 | accuracy : 0.9159\n",
      "step: 646 | accuracy : 0.9048\n",
      "step: 647 | accuracy : 0.9132\n",
      "step: 648 | accuracy : 0.9069\n",
      "step: 649 | accuracy : 0.9104\n",
      "step: 650 | accuracy : 0.9091\n",
      "step: 651 | accuracy : 0.9080\n",
      "step: 652 | accuracy : 0.9158\n",
      "step: 653 | accuracy : 0.9146\n",
      "step: 654 | accuracy : 0.9118\n",
      "step: 655 | accuracy : 0.9088\n",
      "step: 656 | accuracy : 0.9127\n",
      "step: 657 | accuracy : 0.9089\n",
      "step: 658 | accuracy : 0.9080\n",
      "step: 659 | accuracy : 0.9045\n",
      "step: 660 | accuracy : 0.9124\n",
      "step: 661 | accuracy : 0.9147\n",
      "step: 662 | accuracy : 0.9127\n",
      "step: 663 | accuracy : 0.9130\n",
      "step: 664 | accuracy : 0.9080\n",
      "step: 665 | accuracy : 0.9070\n",
      "step: 666 | accuracy : 0.9160\n",
      "step: 667 | accuracy : 0.9145\n",
      "step: 668 | accuracy : 0.9165\n",
      "step: 669 | accuracy : 0.9139\n",
      "step: 670 | accuracy : 0.9100\n",
      "step: 671 | accuracy : 0.9085\n",
      "step: 672 | accuracy : 0.9045\n",
      "step: 673 | accuracy : 0.8951\n",
      "step: 674 | accuracy : 0.8990\n",
      "step: 675 | accuracy : 0.9162\n",
      "step: 676 | accuracy : 0.9052\n",
      "step: 677 | accuracy : 0.9005\n",
      "step: 678 | accuracy : 0.9043\n",
      "step: 679 | accuracy : 0.8999\n",
      "step: 680 | accuracy : 0.8752\n",
      "step: 681 | accuracy : 0.9107\n",
      "step: 682 | accuracy : 0.8993\n",
      "step: 683 | accuracy : 0.8740\n",
      "step: 684 | accuracy : 0.9038\n",
      "step: 685 | accuracy : 0.9090\n",
      "step: 686 | accuracy : 0.9127\n",
      "step: 687 | accuracy : 0.9048\n",
      "step: 688 | accuracy : 0.9109\n",
      "step: 689 | accuracy : 0.9085\n",
      "step: 690 | accuracy : 0.9051\n",
      "step: 691 | accuracy : 0.9145\n",
      "step: 692 | accuracy : 0.9126\n",
      "step: 693 | accuracy : 0.9166\n",
      "step: 694 | accuracy : 0.9147\n",
      "step: 695 | accuracy : 0.9132\n",
      "step: 696 | accuracy : 0.9087\n",
      "step: 697 | accuracy : 0.9035\n",
      "step: 698 | accuracy : 0.9143\n",
      "step: 699 | accuracy : 0.9150\n",
      "step: 700 | accuracy : 0.9161\n",
      "step: 701 | accuracy : 0.9035\n",
      "step: 702 | accuracy : 0.9061\n",
      "step: 703 | accuracy : 0.9056\n",
      "step: 704 | accuracy : 0.9137\n",
      "step: 705 | accuracy : 0.9068\n",
      "step: 706 | accuracy : 0.9010\n",
      "step: 707 | accuracy : 0.9050\n",
      "step: 708 | accuracy : 0.9062\n",
      "step: 709 | accuracy : 0.9159\n",
      "step: 710 | accuracy : 0.9160\n",
      "step: 711 | accuracy : 0.9134\n",
      "step: 712 | accuracy : 0.9110\n",
      "step: 713 | accuracy : 0.9084\n",
      "step: 714 | accuracy : 0.9023\n",
      "step: 715 | accuracy : 0.9156\n",
      "step: 716 | accuracy : 0.9173\n",
      "step: 717 | accuracy : 0.9147\n",
      "step: 718 | accuracy : 0.9132\n",
      "step: 719 | accuracy : 0.9046\n",
      "step: 720 | accuracy : 0.9132\n",
      "step: 721 | accuracy : 0.9042\n",
      "step: 722 | accuracy : 0.9037\n",
      "step: 723 | accuracy : 0.9070\n",
      "step: 724 | accuracy : 0.9096\n",
      "step: 725 | accuracy : 0.8986\n",
      "step: 726 | accuracy : 0.9041\n",
      "step: 727 | accuracy : 0.9154\n",
      "step: 728 | accuracy : 0.9025\n",
      "step: 729 | accuracy : 0.9129\n",
      "step: 730 | accuracy : 0.9123\n",
      "step: 731 | accuracy : 0.9097\n",
      "step: 732 | accuracy : 0.9075\n",
      "step: 733 | accuracy : 0.9183\n",
      "step: 734 | accuracy : 0.9159\n",
      "step: 735 | accuracy : 0.9143\n",
      "step: 736 | accuracy : 0.9153\n",
      "step: 737 | accuracy : 0.9023\n",
      "step: 738 | accuracy : 0.9123\n",
      "step: 739 | accuracy : 0.9172\n",
      "step: 740 | accuracy : 0.9137\n",
      "step: 741 | accuracy : 0.9118\n",
      "step: 742 | accuracy : 0.9170\n",
      "step: 743 | accuracy : 0.9155\n",
      "step: 744 | accuracy : 0.9165\n",
      "step: 745 | accuracy : 0.9178\n",
      "step: 746 | accuracy : 0.9177\n",
      "step: 747 | accuracy : 0.9135\n",
      "step: 748 | accuracy : 0.9141\n",
      "step: 749 | accuracy : 0.9155\n",
      "step: 750 | accuracy : 0.9157\n",
      "step: 751 | accuracy : 0.9069\n",
      "step: 752 | accuracy : 0.9068\n",
      "step: 753 | accuracy : 0.9144\n",
      "step: 754 | accuracy : 0.9141\n",
      "step: 755 | accuracy : 0.9141\n",
      "step: 756 | accuracy : 0.9098\n",
      "step: 757 | accuracy : 0.9131\n",
      "step: 758 | accuracy : 0.8976\n",
      "step: 759 | accuracy : 0.9087\n",
      "step: 760 | accuracy : 0.8928\n",
      "step: 761 | accuracy : 0.9106\n",
      "step: 762 | accuracy : 0.9166\n",
      "step: 763 | accuracy : 0.9159\n",
      "step: 764 | accuracy : 0.9140\n",
      "step: 765 | accuracy : 0.9184\n",
      "step: 766 | accuracy : 0.9053\n",
      "step: 767 | accuracy : 0.9099\n",
      "step: 768 | accuracy : 0.9077\n",
      "step: 769 | accuracy : 0.9092\n",
      "step: 770 | accuracy : 0.9123\n",
      "step: 771 | accuracy : 0.9129\n",
      "step: 772 | accuracy : 0.9100\n",
      "step: 773 | accuracy : 0.9133\n",
      "step: 774 | accuracy : 0.9142\n",
      "step: 775 | accuracy : 0.9135\n",
      "step: 776 | accuracy : 0.9150\n",
      "step: 777 | accuracy : 0.9159\n",
      "step: 778 | accuracy : 0.9164\n",
      "step: 779 | accuracy : 0.9121\n",
      "step: 780 | accuracy : 0.8980\n",
      "step: 781 | accuracy : 0.9122\n",
      "step: 782 | accuracy : 0.9063\n",
      "step: 783 | accuracy : 0.9146\n",
      "step: 784 | accuracy : 0.9035\n",
      "step: 785 | accuracy : 0.8647\n",
      "step: 786 | accuracy : 0.9071\n",
      "step: 787 | accuracy : 0.9124\n",
      "step: 788 | accuracy : 0.9001\n",
      "step: 789 | accuracy : 0.9020\n",
      "step: 790 | accuracy : 0.8991\n",
      "step: 791 | accuracy : 0.9114\n",
      "step: 792 | accuracy : 0.9102\n",
      "step: 793 | accuracy : 0.9144\n",
      "step: 794 | accuracy : 0.9146\n",
      "step: 795 | accuracy : 0.9159\n",
      "step: 796 | accuracy : 0.9115\n",
      "step: 797 | accuracy : 0.9107\n",
      "step: 798 | accuracy : 0.9171\n",
      "step: 799 | accuracy : 0.9148\n",
      "step: 800 | accuracy : 0.9179\n",
      "step: 801 | accuracy : 0.9163\n",
      "step: 802 | accuracy : 0.9147\n",
      "step: 803 | accuracy : 0.9131\n",
      "step: 804 | accuracy : 0.9147\n",
      "step: 805 | accuracy : 0.9144\n",
      "step: 806 | accuracy : 0.9147\n",
      "step: 807 | accuracy : 0.9110\n",
      "step: 808 | accuracy : 0.9153\n",
      "step: 809 | accuracy : 0.9151\n",
      "step: 810 | accuracy : 0.9167\n",
      "step: 811 | accuracy : 0.9159\n",
      "step: 812 | accuracy : 0.9132\n",
      "step: 813 | accuracy : 0.9159\n",
      "step: 814 | accuracy : 0.9121\n",
      "step: 815 | accuracy : 0.9101\n",
      "step: 816 | accuracy : 0.9183\n",
      "step: 817 | accuracy : 0.9154\n",
      "step: 818 | accuracy : 0.9192\n",
      "step: 819 | accuracy : 0.8994\n",
      "step: 820 | accuracy : 0.9011\n",
      "step: 821 | accuracy : 0.9111\n",
      "step: 822 | accuracy : 0.9157\n",
      "step: 823 | accuracy : 0.9143\n",
      "step: 824 | accuracy : 0.9131\n",
      "step: 825 | accuracy : 0.9105\n",
      "step: 826 | accuracy : 0.8973\n",
      "step: 827 | accuracy : 0.9163\n",
      "step: 828 | accuracy : 0.9117\n",
      "step: 829 | accuracy : 0.9062\n",
      "step: 830 | accuracy : 0.9157\n",
      "step: 831 | accuracy : 0.8954\n",
      "step: 832 | accuracy : 0.9051\n",
      "step: 833 | accuracy : 0.8975\n",
      "step: 834 | accuracy : 0.8761\n",
      "step: 835 | accuracy : 0.8938\n",
      "step: 836 | accuracy : 0.9166\n",
      "step: 837 | accuracy : 0.9115\n",
      "step: 838 | accuracy : 0.9182\n",
      "step: 839 | accuracy : 0.8968\n",
      "step: 840 | accuracy : 0.8977\n",
      "step: 841 | accuracy : 0.9109\n",
      "step: 842 | accuracy : 0.9034\n",
      "step: 843 | accuracy : 0.8967\n",
      "step: 844 | accuracy : 0.9060\n",
      "step: 845 | accuracy : 0.9191\n",
      "step: 846 | accuracy : 0.9156\n",
      "step: 847 | accuracy : 0.9159\n",
      "step: 848 | accuracy : 0.9117\n",
      "step: 849 | accuracy : 0.9157\n",
      "step: 850 | accuracy : 0.8960\n",
      "step: 851 | accuracy : 0.9034\n",
      "step: 852 | accuracy : 0.9125\n",
      "step: 853 | accuracy : 0.9115\n",
      "step: 854 | accuracy : 0.9150\n",
      "step: 855 | accuracy : 0.8988\n",
      "step: 856 | accuracy : 0.9068\n",
      "step: 857 | accuracy : 0.8898\n",
      "step: 858 | accuracy : 0.9144\n",
      "step: 859 | accuracy : 0.9170\n",
      "step: 860 | accuracy : 0.9169\n",
      "step: 861 | accuracy : 0.9165\n",
      "step: 862 | accuracy : 0.9153\n",
      "step: 863 | accuracy : 0.9089\n",
      "step: 864 | accuracy : 0.8944\n",
      "step: 865 | accuracy : 0.9147\n",
      "step: 866 | accuracy : 0.9190\n",
      "step: 867 | accuracy : 0.9164\n",
      "step: 868 | accuracy : 0.9106\n",
      "step: 869 | accuracy : 0.9141\n",
      "step: 870 | accuracy : 0.9199\n",
      "step: 871 | accuracy : 0.9206\n",
      "step: 872 | accuracy : 0.9130\n",
      "step: 873 | accuracy : 0.9099\n",
      "step: 874 | accuracy : 0.9161\n",
      "step: 875 | accuracy : 0.9166\n",
      "step: 876 | accuracy : 0.9190\n",
      "step: 877 | accuracy : 0.9127\n",
      "step: 878 | accuracy : 0.9171\n",
      "step: 879 | accuracy : 0.9154\n",
      "step: 880 | accuracy : 0.8985\n",
      "step: 881 | accuracy : 0.9163\n",
      "step: 882 | accuracy : 0.9081\n",
      "step: 883 | accuracy : 0.9010\n",
      "step: 884 | accuracy : 0.9162\n",
      "step: 885 | accuracy : 0.9120\n",
      "step: 886 | accuracy : 0.9143\n",
      "step: 887 | accuracy : 0.9160\n",
      "step: 888 | accuracy : 0.9157\n",
      "step: 889 | accuracy : 0.9044\n",
      "step: 890 | accuracy : 0.9133\n",
      "step: 891 | accuracy : 0.9141\n",
      "step: 892 | accuracy : 0.9142\n",
      "step: 893 | accuracy : 0.9094\n",
      "step: 894 | accuracy : 0.9100\n",
      "step: 895 | accuracy : 0.9116\n",
      "step: 896 | accuracy : 0.9151\n",
      "step: 897 | accuracy : 0.9119\n",
      "step: 898 | accuracy : 0.9193\n",
      "step: 899 | accuracy : 0.9175\n",
      "step: 900 | accuracy : 0.9202\n",
      "step: 901 | accuracy : 0.9168\n",
      "step: 902 | accuracy : 0.8996\n",
      "step: 903 | accuracy : 0.9123\n",
      "step: 904 | accuracy : 0.9141\n",
      "step: 905 | accuracy : 0.9083\n",
      "step: 906 | accuracy : 0.9160\n",
      "step: 907 | accuracy : 0.9099\n",
      "step: 908 | accuracy : 0.9195\n",
      "step: 909 | accuracy : 0.9173\n",
      "step: 910 | accuracy : 0.9053\n",
      "step: 911 | accuracy : 0.9155\n",
      "step: 912 | accuracy : 0.9078\n",
      "step: 913 | accuracy : 0.9185\n",
      "step: 914 | accuracy : 0.9150\n",
      "step: 915 | accuracy : 0.9134\n",
      "step: 916 | accuracy : 0.9130\n",
      "step: 917 | accuracy : 0.9161\n",
      "step: 918 | accuracy : 0.9212\n",
      "step: 919 | accuracy : 0.9193\n",
      "step: 920 | accuracy : 0.9124\n",
      "step: 921 | accuracy : 0.9160\n",
      "step: 922 | accuracy : 0.9095\n",
      "step: 923 | accuracy : 0.9099\n",
      "step: 924 | accuracy : 0.9139\n",
      "step: 925 | accuracy : 0.8739\n",
      "step: 926 | accuracy : 0.8870\n",
      "step: 927 | accuracy : 0.8921\n",
      "step: 928 | accuracy : 0.9146\n",
      "step: 929 | accuracy : 0.9033\n",
      "step: 930 | accuracy : 0.9098\n",
      "step: 931 | accuracy : 0.9182\n",
      "step: 932 | accuracy : 0.9155\n",
      "step: 933 | accuracy : 0.9123\n",
      "step: 934 | accuracy : 0.9096\n",
      "step: 935 | accuracy : 0.9171\n",
      "step: 936 | accuracy : 0.9152\n",
      "step: 937 | accuracy : 0.9068\n",
      "step: 938 | accuracy : 0.9066\n",
      "step: 939 | accuracy : 0.9137\n",
      "step: 940 | accuracy : 0.9013\n",
      "step: 941 | accuracy : 0.9094\n",
      "step: 942 | accuracy : 0.9083\n",
      "step: 943 | accuracy : 0.9065\n",
      "step: 944 | accuracy : 0.9131\n",
      "step: 945 | accuracy : 0.9138\n",
      "step: 946 | accuracy : 0.9189\n",
      "step: 947 | accuracy : 0.9125\n",
      "step: 948 | accuracy : 0.9050\n",
      "step: 949 | accuracy : 0.9154\n",
      "step: 950 | accuracy : 0.9113\n",
      "step: 951 | accuracy : 0.9158\n",
      "step: 952 | accuracy : 0.9188\n",
      "step: 953 | accuracy : 0.9180\n",
      "step: 954 | accuracy : 0.9150\n",
      "step: 955 | accuracy : 0.9130\n",
      "step: 956 | accuracy : 0.9195\n",
      "step: 957 | accuracy : 0.9147\n",
      "step: 958 | accuracy : 0.8961\n",
      "step: 959 | accuracy : 0.9114\n",
      "step: 960 | accuracy : 0.9200\n",
      "step: 961 | accuracy : 0.9109\n",
      "step: 962 | accuracy : 0.9167\n",
      "step: 963 | accuracy : 0.9113\n",
      "step: 964 | accuracy : 0.9182\n",
      "step: 965 | accuracy : 0.9092\n",
      "step: 966 | accuracy : 0.9132\n",
      "step: 967 | accuracy : 0.9158\n",
      "step: 968 | accuracy : 0.9167\n",
      "step: 969 | accuracy : 0.9080\n",
      "step: 970 | accuracy : 0.9149\n",
      "step: 971 | accuracy : 0.9133\n",
      "step: 972 | accuracy : 0.9085\n",
      "step: 973 | accuracy : 0.9096\n",
      "step: 974 | accuracy : 0.9086\n",
      "step: 975 | accuracy : 0.9145\n",
      "step: 976 | accuracy : 0.9058\n",
      "step: 977 | accuracy : 0.9041\n",
      "step: 978 | accuracy : 0.9071\n",
      "step: 979 | accuracy : 0.9104\n",
      "step: 980 | accuracy : 0.9154\n",
      "step: 981 | accuracy : 0.8985\n",
      "step: 982 | accuracy : 0.9147\n",
      "step: 983 | accuracy : 0.9153\n",
      "step: 984 | accuracy : 0.9192\n",
      "step: 985 | accuracy : 0.9188\n",
      "step: 986 | accuracy : 0.9173\n",
      "step: 987 | accuracy : 0.9185\n",
      "step: 988 | accuracy : 0.9142\n",
      "step: 989 | accuracy : 0.9182\n",
      "step: 990 | accuracy : 0.9153\n",
      "step: 991 | accuracy : 0.9178\n",
      "step: 992 | accuracy : 0.9027\n",
      "step: 993 | accuracy : 0.9162\n",
      "step: 994 | accuracy : 0.9133\n",
      "step: 995 | accuracy : 0.9168\n",
      "step: 996 | accuracy : 0.9174\n",
      "step: 997 | accuracy : 0.9159\n",
      "step: 998 | accuracy : 0.9031\n",
      "step: 999 | accuracy : 0.8981\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "y = tf.nn.softmax( tf.matmul(x,W) + b )\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# choose node\n",
    "loss_hist = tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "accur_hist = tf.summary.scalar(\"valid_hist\", accuracy)\n",
    "\n",
    "# merge all\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# set writer of train and test\n",
    "train_writer = tf.summary.FileWriter(\"./sample/train\", sess.graph)\n",
    "test_writer = tf.summary.FileWriter(\"./sample/test\", sess.graph)\n",
    "\n",
    "for step in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    \n",
    "    # run loss_hist and train_step\n",
    "    summary, _ = sess.run([loss_hist, train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "    # add summary of loss hist\n",
    "    train_writer.add_summary(summary, step)\n",
    "\n",
    "    #run accur_hist and accuracy\n",
    "    summary, accuracy_ = sess.run([accur_hist, accuracy], feed_dict={x:mnist.test.images,\n",
    "                                        y_:mnist.test.labels})\n",
    "    # add summary of accuracy hist\n",
    "    test_writer.add_summary(summary, step)\n",
    "\n",
    "    print('step: {:01d} | accuracy : {:.4f}'.format(step,float(accuracy_)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/SEC/Desktop/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"C:/Users/SEC/Desktop/MNIST_data\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.convert_to_tensor(mnist.train.images).get_shape())\n",
    "print(tf.convert_to_tensor(mnist.train.labels).get_shape())\n",
    "\n",
    "# You can freeze your layer by setting trainable=False\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y_ = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "learning_rate = tf.placeholder('float', [])\n",
    "\n",
    "y = tf.nn.softmax( tf.matmul(x,W) + b )\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver({\"W\": W})\n",
    "\n",
    "lr = 0.01\n",
    "for step in range(200):\n",
    "    \n",
    "    # saver.restore(sess, './model/model_100.ckpt')\n",
    "    \n",
    "    if(step % 100 == 0 and step != 0):\n",
    "        save_path = \"./model/model_\" + str(step) + '.ckpt'\n",
    "        saver.save(sess, save_path)\n",
    "        lr = lr / 10\n",
    "    \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    \n",
    "    # run loss_hist and train_step\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, lr: learning_rate})\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "\n",
    "    print('step: {:01d} | accuracy : {:.4f}'.format(step,float(accuracy_)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
