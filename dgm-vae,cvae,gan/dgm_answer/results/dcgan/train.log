n_ch 32, n_noise 50
step 100, train batch D loss 4.784230, G loss 0.557157
step 200, train batch D loss 7.709542, G loss 0.577091
step 300, train batch D loss 10.687740, G loss 0.633753
step 400, train batch D loss 0.647308, G loss 2.599258
step 500, train batch D loss 2.024898, G loss 0.307767
step 600, train batch D loss 0.560414, G loss 1.236810
step 700, train batch D loss 0.856472, G loss 0.967553
step 800, train batch D loss 1.086069, G loss 0.884634
step 900, train batch D loss 1.108419, G loss 0.893791
step 1000, train batch D loss 1.134917, G loss 0.836004
step 1100, train batch D loss 1.115415, G loss 0.912326
step 1200, train batch D loss 1.146710, G loss 0.862512
step 1300, train batch D loss 1.137155, G loss 0.828876
step 1400, train batch D loss 1.175407, G loss 0.853389
step 1500, train batch D loss 1.165601, G loss 0.778210
step 1600, train batch D loss 1.197230, G loss 0.743403
step 1700, train batch D loss 1.185159, G loss 0.755560
step 1800, train batch D loss 1.181720, G loss 0.815279
step 1900, train batch D loss 1.155287, G loss 0.796169
step 2000, train batch D loss 1.204125, G loss 0.845337
step 2100, train batch D loss 1.115556, G loss 0.821654
step 2200, train batch D loss 1.176304, G loss 0.832639
step 2300, train batch D loss 1.200428, G loss 0.825008
step 2400, train batch D loss 1.210011, G loss 0.801787
step 2500, train batch D loss 1.219082, G loss 0.786687
step 2600, train batch D loss 1.218439, G loss 0.760717
step 2700, train batch D loss 1.164231, G loss 0.777137
step 2800, train batch D loss 1.194464, G loss 0.780229
step 2900, train batch D loss 1.167479, G loss 0.819563
step 3000, train batch D loss 1.186608, G loss 0.805993
step 3100, train batch D loss 1.216556, G loss 0.808977
step 3200, train batch D loss 1.217405, G loss 0.768999
step 3300, train batch D loss 1.240666, G loss 0.783687
step 3400, train batch D loss 1.233830, G loss 0.774766
step 3500, train batch D loss 1.176687, G loss 0.814355
step 3600, train batch D loss 1.227239, G loss 0.806840
step 3700, train batch D loss 1.196847, G loss 0.873668
step 3800, train batch D loss 1.253160, G loss 0.752836
step 3900, train batch D loss 1.181872, G loss 0.872129
step 4000, train batch D loss 1.272243, G loss 0.727977
step 4100, train batch D loss 1.190709, G loss 0.835074
step 4200, train batch D loss 1.229220, G loss 0.766893
step 4300, train batch D loss 1.269106, G loss 0.818224
step 4400, train batch D loss 1.257099, G loss 0.834372
step 4500, train batch D loss 1.267547, G loss 0.792559
step 4600, train batch D loss 1.229814, G loss 0.793039
step 4700, train batch D loss 1.269521, G loss 0.805335
step 4800, train batch D loss 1.262801, G loss 0.781200
step 4900, train batch D loss 1.201754, G loss 0.788835
step 5000, train batch D loss 1.145459, G loss 0.850012
step 5100, train batch D loss 1.257791, G loss 0.766608
step 5200, train batch D loss 1.253353, G loss 0.772640
step 5300, train batch D loss 1.234502, G loss 0.786669
step 5400, train batch D loss 1.260432, G loss 0.757053
step 5500, train batch D loss 1.225104, G loss 0.799309
step 5600, train batch D loss 1.195542, G loss 0.841600
step 5700, train batch D loss 1.230361, G loss 0.800966
step 5800, train batch D loss 1.184078, G loss 0.788145
step 5900, train batch D loss 1.201972, G loss 0.815738
step 6000, train batch D loss 1.197324, G loss 0.806432
step 6100, train batch D loss 1.217805, G loss 0.848895
step 6200, train batch D loss 1.165402, G loss 0.888013
step 6300, train batch D loss 1.195226, G loss 0.786979
step 6400, train batch D loss 1.221120, G loss 0.813088
step 6500, train batch D loss 1.181157, G loss 0.805479
step 6600, train batch D loss 1.189190, G loss 0.847262
step 6700, train batch D loss 1.208363, G loss 0.797197
step 6800, train batch D loss 1.205977, G loss 0.801486
step 6900, train batch D loss 1.207019, G loss 0.775190
step 7000, train batch D loss 1.238817, G loss 0.869054
step 7100, train batch D loss 1.251625, G loss 0.794959
step 7200, train batch D loss 1.157475, G loss 0.820868
step 7300, train batch D loss 1.217237, G loss 0.764900
step 7400, train batch D loss 1.195471, G loss 0.825636
step 7500, train batch D loss 1.159006, G loss 0.770811
step 7600, train batch D loss 1.259504, G loss 0.739270
step 7700, train batch D loss 1.207870, G loss 0.790170
step 7800, train batch D loss 1.242876, G loss 0.752735
step 7900, train batch D loss 1.200811, G loss 0.849276
step 8000, train batch D loss 1.201440, G loss 0.835895
step 8100, train batch D loss 1.222744, G loss 0.865959
step 8200, train batch D loss 1.137193, G loss 0.847605
step 8300, train batch D loss 1.196417, G loss 0.827783
step 8400, train batch D loss 1.144987, G loss 0.875398
step 8500, train batch D loss 1.188233, G loss 0.828527
step 8600, train batch D loss 1.189941, G loss 0.849929
step 8700, train batch D loss 1.185091, G loss 0.789227
step 8800, train batch D loss 1.132050, G loss 0.927693
step 8900, train batch D loss 1.222425, G loss 0.825531
step 9000, train batch D loss 1.203735, G loss 0.868498
step 9100, train batch D loss 1.193508, G loss 0.868863
step 9200, train batch D loss 1.171230, G loss 0.854979
step 9300, train batch D loss 1.114650, G loss 0.897229
step 9400, train batch D loss 1.137243, G loss 0.895919
step 9500, train batch D loss 1.162371, G loss 0.876726
step 9600, train batch D loss 1.148218, G loss 0.883078
step 9700, train batch D loss 1.130420, G loss 1.005666
step 9800, train batch D loss 1.169516, G loss 0.847867
step 9900, train batch D loss 1.136194, G loss 0.917089
step 10000, train batch D loss 1.112860, G loss 0.932908
step 10100, train batch D loss 1.172428, G loss 0.890673
step 10200, train batch D loss 1.128739, G loss 0.892303
step 10300, train batch D loss 1.208638, G loss 0.801014
step 10400, train batch D loss 1.184989, G loss 0.811001
step 10500, train batch D loss 1.093183, G loss 0.920759
step 10600, train batch D loss 1.144578, G loss 0.920211
step 10700, train batch D loss 1.154382, G loss 0.911283
step 10800, train batch D loss 1.096045, G loss 0.904440
step 10900, train batch D loss 1.151931, G loss 0.810197
step 11000, train batch D loss 1.132684, G loss 0.878293
step 11100, train batch D loss 1.106658, G loss 0.980561
step 11200, train batch D loss 1.076073, G loss 0.965759
step 11300, train batch D loss 1.070436, G loss 0.986513
step 11400, train batch D loss 1.110037, G loss 0.970052
step 11500, train batch D loss 1.168256, G loss 0.894059
step 11600, train batch D loss 1.047092, G loss 1.083591
step 11700, train batch D loss 1.094329, G loss 0.978493
step 11800, train batch D loss 1.091941, G loss 0.964527
step 11900, train batch D loss 1.062824, G loss 0.972440
step 12000, train batch D loss 1.103622, G loss 0.968845
step 12100, train batch D loss 1.202779, G loss 0.957288
step 12200, train batch D loss 1.181319, G loss 0.884826
step 12300, train batch D loss 1.059727, G loss 0.992382
step 12400, train batch D loss 1.140752, G loss 1.028569
step 12500, train batch D loss 1.081842, G loss 0.945708
step 12600, train batch D loss 1.117247, G loss 0.917910
step 12700, train batch D loss 1.027661, G loss 0.997983
step 12800, train batch D loss 1.071328, G loss 0.920908
step 12900, train batch D loss 1.036947, G loss 0.922244
step 13000, train batch D loss 1.065284, G loss 0.964932
step 13100, train batch D loss 1.088723, G loss 1.043531
step 13200, train batch D loss 1.141803, G loss 1.008808
step 13300, train batch D loss 1.073652, G loss 0.916634
step 13400, train batch D loss 1.025049, G loss 1.064884
step 13500, train batch D loss 1.112505, G loss 1.048237
step 13600, train batch D loss 1.073707, G loss 1.044618
step 13700, train batch D loss 1.079893, G loss 0.932690
step 13800, train batch D loss 1.064459, G loss 0.976347
step 13900, train batch D loss 1.006719, G loss 1.042593
step 14000, train batch D loss 1.075509, G loss 0.877959
step 14100, train batch D loss 1.043465, G loss 1.004193
step 14200, train batch D loss 0.996259, G loss 0.960907
step 14300, train batch D loss 1.135003, G loss 0.902758
step 14400, train batch D loss 1.001102, G loss 0.967318
step 14500, train batch D loss 1.024843, G loss 1.052704
step 14600, train batch D loss 1.124364, G loss 0.851771
step 14700, train batch D loss 1.031114, G loss 1.123123
step 14800, train batch D loss 1.076521, G loss 1.055404
step 14900, train batch D loss 1.037361, G loss 0.927367
step 15000, train batch D loss 1.064334, G loss 0.981002
step 15100, train batch D loss 1.055628, G loss 1.089080
step 15200, train batch D loss 1.068458, G loss 0.978003
step 15300, train batch D loss 1.128401, G loss 0.951852
step 15400, train batch D loss 1.114168, G loss 0.861016
step 15500, train batch D loss 0.988774, G loss 1.208249
step 15600, train batch D loss 0.937262, G loss 1.048477
step 15700, train batch D loss 1.034831, G loss 1.182662
step 15800, train batch D loss 0.976956, G loss 1.199436
step 15900, train batch D loss 0.989545, G loss 1.042581
step 16000, train batch D loss 0.971230, G loss 1.101404
step 16100, train batch D loss 1.010427, G loss 1.089995
step 16200, train batch D loss 1.067875, G loss 1.056577
step 16300, train batch D loss 0.992218, G loss 1.125920
step 16400, train batch D loss 0.950805, G loss 1.110896
step 16500, train batch D loss 1.058625, G loss 1.041066
step 16600, train batch D loss 0.976720, G loss 1.248269
step 16700, train batch D loss 0.945082, G loss 1.099863
step 16800, train batch D loss 1.004721, G loss 1.014372
step 16900, train batch D loss 0.976048, G loss 1.050664
step 17000, train batch D loss 0.973962, G loss 1.184532
step 17100, train batch D loss 0.971124, G loss 1.179575
step 17200, train batch D loss 1.019931, G loss 0.983079
step 17300, train batch D loss 1.021437, G loss 1.095691
step 17400, train batch D loss 0.949001, G loss 1.067677
step 17500, train batch D loss 0.943261, G loss 1.039862
step 17600, train batch D loss 0.992590, G loss 1.022842
step 17700, train batch D loss 1.014672, G loss 1.093946
step 17800, train batch D loss 1.042721, G loss 1.111162
step 17900, train batch D loss 1.016141, G loss 1.146765
step 18000, train batch D loss 0.979983, G loss 1.097554
step 18100, train batch D loss 0.933901, G loss 1.200325
step 18200, train batch D loss 0.939585, G loss 1.160564
step 18300, train batch D loss 0.974979, G loss 1.161628
step 18400, train batch D loss 0.974699, G loss 1.122710
step 18500, train batch D loss 1.043920, G loss 0.999835
step 18600, train batch D loss 0.930273, G loss 1.270714
step 18700, train batch D loss 1.007284, G loss 1.151625
step 18800, train batch D loss 0.889538, G loss 1.286996
step 18900, train batch D loss 0.975001, G loss 1.006515
step 19000, train batch D loss 0.966158, G loss 1.097847
step 19100, train batch D loss 1.033582, G loss 1.007281
step 19200, train batch D loss 0.949638, G loss 1.259366
step 19300, train batch D loss 0.922479, G loss 1.204658
step 19400, train batch D loss 0.976162, G loss 1.165788
step 19500, train batch D loss 0.921511, G loss 1.104635
step 19600, train batch D loss 1.005414, G loss 0.931424
step 19700, train batch D loss 0.897596, G loss 1.388939
step 19800, train batch D loss 0.924063, G loss 1.031155
step 19900, train batch D loss 0.915925, G loss 1.171372
step 20000, train batch D loss 0.891899, G loss 1.267763
